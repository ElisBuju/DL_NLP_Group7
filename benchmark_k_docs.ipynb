{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch faiss-cpu faiss-gpu-cu12 tqdm numpy==1.26.4\n",
    "\n",
    "!pip install pyarrow==14.0.1 datasets==2.14.6 transformers==4.35.2 accelerate==0.24.1\n",
    "\n",
    "!pip -q install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall faiss-cpu -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    RagTokenForGeneration\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP & HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def calculate_em(predictions, references):\n",
    "    total_em = 0\n",
    "    for pred, refs in zip(predictions, references):\n",
    "        if any(exact_match_score(pred, gt) for gt in refs):\n",
    "            total_em += 1\n",
    "    return 100 * (total_em / len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. RAG MODEL MANAGER\n",
    "# ==========================================\n",
    "\n",
    "class RAGModelManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        rag_type,  # \"sequence\" or \"token\"\n",
    "        n_docs_init=5,\n",
    "        use_dummy=False,\n",
    "        index_name=\"exact\",\n",
    "        num_beams=1,\n",
    "        max_new_tokens=16,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.rag_type = rag_type\n",
    "        self.n_docs = n_docs_init\n",
    "        self.num_beams = num_beams\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "        print(f\"Loading Model: {model_name} ({rag_type})...\")\n",
    "        self.tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "        self.retriever = RagRetriever.from_pretrained(\n",
    "            model_name,\n",
    "            index_name=index_name,\n",
    "            use_dummy_dataset=use_dummy\n",
    "        )\n",
    "\n",
    "        if rag_type == \"token\":\n",
    "            self.model = RagTokenForGeneration.from_pretrained(\n",
    "                model_name, retriever=self.retriever\n",
    "            ).to(device)\n",
    "        else:\n",
    "            self.model = RagSequenceForGeneration.from_pretrained(\n",
    "                model_name, retriever=self.retriever\n",
    "            ).to(device)\n",
    "\n",
    "        # Initial config\n",
    "        self.set_n_docs(n_docs_init)\n",
    "        self.model.eval()\n",
    "\n",
    "    def set_n_docs(self, n_docs: int):\n",
    "        \"\"\"Updates the number of retrieved documents (k) dynamically.\"\"\"\n",
    "        self.n_docs = n_docs\n",
    "        self.model.config.n_docs = n_docs\n",
    "        if hasattr(self.retriever, \"n_docs\"):\n",
    "            self.retriever.n_docs = n_docs\n",
    "        if hasattr(self.retriever, \"config\"):\n",
    "            self.retriever.config.n_docs = n_docs\n",
    "\n",
    "    def generate_answers(self, questions, batch_size=4):\n",
    "        self.model.eval()\n",
    "        all_answers = []\n",
    "        \n",
    "        # Batch processing\n",
    "        for i in tqdm(range(0, len(questions), batch_size), desc=f\"Generating (k={self.n_docs})\"):\n",
    "            batch_questions = questions[i: i + batch_size]\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                batch_questions,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    n_docs=self.n_docs,\n",
    "                    num_beams=self.num_beams,\n",
    "                    max_new_tokens=self.max_new_tokens\n",
    "                )\n",
    "\n",
    "            batch_answers = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            all_answers.extend(batch_answers)\n",
    "\n",
    "        return all_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. EXPERIMENT CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# Settings\n",
    "USE_DUMMY = False  # Set False for real evaluation (downloads large index)\n",
    "NUM_SAMPLES = None   # Use None for full dataset, or int (e.g., 100) for testing\n",
    "K_VALUES = [3, 5, 7, 10, 20, 30] # The list of 'k' values to evaluate\n",
    "BATCH_SIZE = 8\n",
    "MAX_LOOP_TIME = None        # e.g., 30 minutes per model\n",
    "MAX_ITER_TIME = 8400        # e.g., 300 for 5 minutes per k (or None to disable)\n",
    "\n",
    "\n",
    "# Load Data Once\n",
    "print(\"Loading Natural Questions Validation Set...\")\n",
    "dataset = load_dataset(\"nq_open\", split=\"validation\")\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for row in dataset:\n",
    "    questions.append(row['question'])\n",
    "    answers.append(row['answer'])\n",
    "    if NUM_SAMPLES and len(questions) >= NUM_SAMPLES:\n",
    "        break\n",
    "\n",
    "print(f\"Loaded {len(questions)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. RUN EVALUATION LOOP\n",
    "# ==========================================\n",
    "\n",
    "results = {\n",
    "    \"rag-token\": {\"scores\": [], \"k\": K_VALUES},\n",
    "    \"rag-sequence\": {\"scores\": [], \"k\": K_VALUES}\n",
    "}\n",
    "\n",
    "print(\"\\n=== Evaluating RAG-Token ===\")\n",
    "rag_token_mgr = RAGModelManager(\n",
    "    model_name=\"facebook/rag-token-nq\",\n",
    "    rag_type=\"token\",\n",
    "    use_dummy=USE_DUMMY\n",
    ")\n",
    "\n",
    "loop_start_time = time.time()\n",
    "\n",
    "for k in K_VALUES:\n",
    "    iter_start_time = time.time()\n",
    "\n",
    "    # ---- Global loop timeout ----\n",
    "    elapsed_loop_time = time.time() - loop_start_time\n",
    "    if MAX_LOOP_TIME is not None and elapsed_loop_time > MAX_LOOP_TIME:\n",
    "        print(\n",
    "            f\"[INTERRUPTED] Loop exceeded max time \"\n",
    "            f\"({elapsed_loop_time:.1f}s > {MAX_LOOP_TIME}s)\"\n",
    "        )\n",
    "        break\n",
    "\n",
    "    print(f\"\\nProcessing k={k} for RAG-Token...\")\n",
    "\n",
    "    rag_token_mgr.set_n_docs(k)\n",
    "    predictions = rag_token_mgr.generate_answers(\n",
    "        questions, batch_size=BATCH_SIZE\n",
    "    )\n",
    "    score = calculate_em(predictions, answers)\n",
    "    results[\"rag-token\"][\"scores\"].append(score)\n",
    "\n",
    "    iter_time = time.time() - iter_start_time\n",
    "    print(\n",
    "        f\"RAG-Token (k={k}) EM: {score:.2f}% \"\n",
    "        f\"| Iter time: {iter_time:.1f}s\"\n",
    "    )\n",
    "\n",
    "    # ---- Per-iteration timeout (optional) ----\n",
    "    if MAX_ITER_TIME is not None and iter_time > MAX_ITER_TIME:\n",
    "        print(\n",
    "            f\"[INTERRUPTED] RAG-Token iteration exceeded \"\n",
    "            f\"max iteration time ({iter_time:.1f}s > {MAX_ITER_TIME}s)\"\n",
    "        )\n",
    "        break\n",
    "\n",
    "\n",
    "# Free memory before next model\n",
    "del rag_token_mgr\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n=== Evaluating RAG-Sequence ===\")\n",
    "rag_seq_mgr = RAGModelManager(\n",
    "    model_name=\"facebook/rag-sequence-nq\",\n",
    "    rag_type=\"sequence\",\n",
    "    use_dummy=USE_DUMMY\n",
    ")\n",
    "\n",
    "loop_start_time = time.time()\n",
    "\n",
    "for k in K_VALUES:\n",
    "    iter_start_time = time.time()\n",
    "\n",
    "    # ---- Global loop timeout ----\n",
    "    elapsed_loop_time = time.time() - loop_start_time\n",
    "    if MAX_LOOP_TIME is not None and elapsed_loop_time > MAX_LOOP_TIME:\n",
    "        print(\n",
    "            f\"[INTERRUPTED] Loop exceeded max time \"\n",
    "            f\"({elapsed_loop_time:.1f}s > {MAX_LOOP_TIME}s)\"\n",
    "        )\n",
    "        break\n",
    "\n",
    "    print(f\"\\nProcessing k={k} for RAG-Sequence...\")\n",
    "\n",
    "    rag_seq_mgr.set_n_docs(k)\n",
    "    predictions = rag_seq_mgr.generate_answers(\n",
    "        questions, batch_size=BATCH_SIZE\n",
    "    )\n",
    "    score = calculate_em(predictions, answers)\n",
    "    results[\"rag-sequence\"][\"scores\"].append(score)\n",
    "\n",
    "    iter_time = time.time() - iter_start_time\n",
    "    print(\n",
    "        f\"RAG-Sequence (k={k}) EM: {score:.2f}% \"\n",
    "        f\"| Iter time: {iter_time:.1f}s\"\n",
    "    )\n",
    "\n",
    "    # ---- Per-iteration timeout (optional) ----\n",
    "    if MAX_ITER_TIME is not None and iter_time > MAX_ITER_TIME:\n",
    "        print(\n",
    "            f\"[INTERRUPTED] RAG-Sequence iteration exceeded \"\n",
    "            f\"max iteration time ({iter_time:.1f}s > {MAX_ITER_TIME}s)\"\n",
    "        )\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. PLOTTING RESULTS\n",
    "# ==========================================\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot RAG-Token\n",
    "plt.plot(\n",
    "    results[\"rag-token\"][\"k\"], \n",
    "    results[\"rag-token\"][\"scores\"], \n",
    "    marker='o', \n",
    "    label='RAG-Token', \n",
    "    linestyle='-', \n",
    "    color='blue'\n",
    ")\n",
    "\n",
    "# Plot RAG-Sequence\n",
    "plt.plot(\n",
    "    results[\"rag-sequence\"][\"k\"], \n",
    "    results[\"rag-sequence\"][\"scores\"], \n",
    "    marker='s', \n",
    "    label='RAG-Sequence', \n",
    "    linestyle='--', \n",
    "    color='green'\n",
    ")\n",
    "\n",
    "plt.title(f'RAG Performance on NQ (Exact Match) vs Retrieved Docs (k)\\n(Samples: {len(questions)})')\n",
    "plt.xlabel('Number of Retrieved Documents (k)')\n",
    "plt.ylabel('Exact Match (EM) Score %')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xticks(K_VALUES)  # Ensure all k values are shown on x-axis\n",
    "\n",
    "# Save plot or show\n",
    "plt.savefig(\"k_retrieval_performance.png\")\n",
    "print(\"Plot saved as 'k_retrieval_performance.png'\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
