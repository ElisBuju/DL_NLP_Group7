{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c082cae5-6267-4834-a90b-e29e6f3a1a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/micromamba/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/micromamba/lib/python3.11/site-packages (1.13.2)\n",
      "Requirement already satisfied: faiss-gpu-cu12 in /opt/micromamba/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: tqdm in /opt/micromamba/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in /opt/micromamba/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/micromamba/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/micromamba/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/micromamba/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/micromamba/lib/python3.11/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/micromamba/lib/python3.11/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/micromamba/lib/python3.11/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/micromamba/lib/python3.11/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/micromamba/lib/python3.11/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/micromamba/lib/python3.11/site-packages (from triton==3.4.0->torch) (80.9.0)\n",
      "Requirement already satisfied: packaging in /opt/micromamba/lib/python3.11/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/micromamba/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pyarrow==14.0.1 in /opt/micromamba/lib/python3.11/site-packages (14.0.1)\n",
      "Requirement already satisfied: datasets==2.14.6 in /opt/micromamba/lib/python3.11/site-packages (2.14.6)\n",
      "Requirement already satisfied: transformers==4.35.2 in /opt/micromamba/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: accelerate==0.24.1 in /opt/micromamba/lib/python3.11/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/micromamba/lib/python3.11/site-packages (from pyarrow==14.0.1) (1.26.4)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/micromamba/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.14.6) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (3.12.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (0.35.0)\n",
      "Requirement already satisfied: packaging in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (6.0.2)\n",
      "Requirement already satisfied: filelock in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (3.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (0.6.2)\n",
      "Requirement already satisfied: psutil in /opt/micromamba/lib/python3.11/site-packages (from accelerate==0.24.1) (6.1.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/micromamba/lib/python3.11/site-packages (from accelerate==0.24.1) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/micromamba/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/micromamba/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (1.1.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.14.6) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.6) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.6) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.6) (2025.8.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/micromamba/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.10.0->accelerate==0.24.1) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/micromamba/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.24.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate==0.24.1) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/micromamba/lib/python3.11/site-packages (from pandas->datasets==2.14.6) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/micromamba/lib/python3.11/site-packages (from pandas->datasets==2.14.6) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/micromamba/lib/python3.11/site-packages (from pandas->datasets==2.14.6) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/micromamba/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch faiss-cpu faiss-gpu-cu12 tqdm numpy==1.26.4\n",
    "\n",
    "!pip install pyarrow==14.0.1 datasets==2.14.6 transformers==4.35.2 accelerate==0.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16857394-90da-41b5-892c-5c67888890f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    RagTokenForGeneration\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP & HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dabab9f3-3a3f-4d56-b6ba-e80a6ed423b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def calculate_em(predictions, references):\n",
    "    total_em = 0\n",
    "    for pred, refs in zip(predictions, references):\n",
    "        if any(exact_match_score(pred, gt) for gt in refs):\n",
    "            total_em += 1\n",
    "    return 100 * (total_em / len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7be2ff-f99f-4dc6-ba13-1e357febd4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalRAGModelManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        rag_type,        \n",
    "        n_docs=5,\n",
    "        index_name=\"exact\",\n",
    "    ):\n",
    "        print(f\"Initializing RAG Manager...\")\n",
    "        self.tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "        self.retriever = RagRetriever.from_pretrained(model_name, index_name=index_name, use_dummy_dataset=False)\n",
    "\n",
    "        print(f\"[DEBUG] Loading Model from {model_name}...\")\n",
    "        ModelClass = RagTokenForGeneration if rag_type == \"token\" else RagSequenceForGeneration\n",
    "        self.model = ModelClass.from_pretrained(model_name, retriever=self.retriever).to(device)\n",
    "\n",
    "        self.model.config.n_docs = n_docs\n",
    "        self.model.eval()\n",
    "        print(\"[DEBUG] Model loaded successfully!\")\n",
    "\n",
    "    def set_n_docs(self, n_docs: int):\n",
    "        self.n_docs = n_docs\n",
    "        self.model.config.n_docs = n_docs\n",
    "        if hasattr(self.retriever, \"n_docs\"):\n",
    "            self.retriever.n_docs = n_docs\n",
    "        if hasattr(self.retriever, \"config\"):\n",
    "            self.retriever.config.n_docs = n_docs\n",
    "\n",
    "    def generate_answers(self, questions, batch_size=4):\n",
    "        all_answers = []\n",
    "        for i in tqdm(range(0, len(questions), batch_size), desc=f\"Generating (k={self.n_docs})\"):\n",
    "            batch_questions = questions[i: i + batch_size]\n",
    "            inputs = self.tokenizer(\n",
    "                batch_questions,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    n_docs=self.model.config.n_docs,\n",
    "                    num_beams=1,\n",
    "                    max_new_tokens=16,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            all_answers.extend(self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n",
    "        return all_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8e9f06-7035-48f5-aaf8-e063a0edcc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WebQuestions Test Set...\n",
      "Loaded 2032 test samples from WebQuestions.\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_TOKEN_CHECKPOINT = \"WQ_models/facebook_rag-token-nq__token__wq_ft__nDocs10__20260117_163413\" \n",
    "PATH_TO_SEQUENCE_CHECKPOINT = \"WQ_models/facebook_rag-sequence-nq__sequence__wq_ft__nDocs10__20260118_124326\"\n",
    "\n",
    "K_VALUES = [3, 5, 7, 10, 20, 30]\n",
    "NUM_SAMPLES = None\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Load data\n",
    "print(\"Loading WebQuestions Test Set...\")\n",
    "dataset = load_dataset(\"stanfordnlp/web_questions\", split=\"test\")\n",
    "\n",
    "questions = [row['question'] for row in dataset]\n",
    "answers = [row['answers'] for row in dataset]\n",
    "\n",
    "print(f\"Loaded {len(questions)} test samples from WebQuestions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e32f1c4-c188-404a-a39f-1297d310af39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Fine-Tuned RAG-Token ===\n",
      "Initializing RAG Manager...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/66fd9b80f51375c02cd9010050e781ed3e8f759e868f690c31b2686a7a0eeb5c (last modified on Thu Jan 15 21:07:41 2026) since it couldn't be found locally at wiki_dpr., or remotely on the Hugging Face Hub.\n",
      "/opt/micromamba/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/66fd9b80f51375c02cd9010050e781ed3e8f759e868f690c31b2686a7a0eeb5c (last modified on Thu Jan 15 21:07:41 2026) since it couldn't be found locally at wiki_dpr., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loading Model from WQ_models/facebook_rag-token-nq__token__wq_ft__nDocs10__20260117_163413...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Model loaded successfully!\n",
      "\n",
      "Processing k=3 for RAG-Token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (k=3):   0%|          | 0/254 [00:00<?, ?it/s]/opt/micromamba/lib/python3.11/site-packages/transformers/generation/utils.py:2465: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.\n",
      "  warnings.warn(\n",
      "Generating (k=3): 100%|██████████| 254/254 [07:00<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ Score (k=3): 31.74% (Time: 420.1s)\n",
      "\n",
      "Processing k=5 for RAG-Token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (k=5): 100%|██████████| 254/254 [12:34<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ Score (k=5): 32.04% (Time: 754.9s)\n",
      "\n",
      "Processing k=7 for RAG-Token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (k=7): 100%|██████████| 254/254 [17:28<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ Score (k=7): 32.53% (Time: 1048.2s)\n",
      "\n",
      "Processing k=10 for RAG-Token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (k=10): 100%|██████████| 254/254 [22:11<00:00,  5.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ Score (k=10): 32.63% (Time: 1331.8s)\n",
      "\n",
      "Processing k=20 for RAG-Token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (k=20): 100%|██████████| 254/254 [43:37<00:00, 10.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ Score (k=20): 32.68% (Time: 2617.2s)\n",
      "\n",
      "Processing k=30 for RAG-Token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (k=30): 100%|██████████| 254/254 [1:06:02<00:00, 15.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ Score (k=30): 32.82% (Time: 3962.7s)\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"rag-token\": {\"scores\": [], \"k\": K_VALUES},\n",
    "    \"rag-sequence\": {\"scores\": [], \"k\": K_VALUES}\n",
    "}\n",
    "\n",
    "# Evaluate Fine-Tuned RAG-TOKEN\n",
    "if PATH_TO_TOKEN_CHECKPOINT:\n",
    "    print(f\"\\n=== Evaluating Fine-Tuned RAG-Token ===\")\n",
    "    rag_token_mgr = LocalRAGModelManager(\n",
    "        model_name=PATH_TO_TOKEN_CHECKPOINT,\n",
    "        rag_type=\"token\"\n",
    "    )\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        print(f\"\\nProcessing k={k} for RAG-Token...\")\n",
    "        rag_token_mgr.set_n_docs(k)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        predictions = rag_token_mgr.generate_answers(questions, batch_size=BATCH_SIZE)\n",
    "        score = calculate_em(predictions, answers)\n",
    "        \n",
    "        results[\"rag-token\"][\"scores\"].append(score)\n",
    "        with open(\"benchmark_wq_k_results.json\", \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "        print(f\"WQ Score (k={k}): {score:.2f}% (Time: {time.time()-start_time:.1f}s)\")\n",
    "\n",
    "    # Clean up GPU memory\n",
    "    del rag_token_mgr\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipping Token Model (No path provided)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a5281ff-f858-4930-b02c-49f0209d118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Fine-Tuned RAG-Sequence ===\n",
      "Initializing RAG Manager...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/66fd9b80f51375c02cd9010050e781ed3e8f759e868f690c31b2686a7a0eeb5c (last modified on Thu Jan 15 21:07:41 2026) since it couldn't be found locally at wiki_dpr., or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/66fd9b80f51375c02cd9010050e781ed3e8f759e868f690c31b2686a7a0eeb5c (last modified on Thu Jan 15 21:07:41 2026) since it couldn't be found locally at wiki_dpr., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loading Model from WQ_models/facebook_rag-sequence-nq__sequence__wq_ft__nDocs10__20260118_124326...\n",
      "[DEBUG] Model loaded successfully!\n",
      "\n",
      "Processing k=3 for RAG-Sequence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (k=3):   0%|          | 0/254 [00:00<?, ?it/s]/opt/micromamba/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Generating (k=3): 100%|██████████| 254/254 [14:21<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ Score (k=3): 33.56% (Time: 861.7s)\n",
      "\n",
      "Processing k=5 for RAG-Sequence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (k=5): 100%|██████████| 254/254 [27:44<00:00,  6.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ Score (k=5): 35.09% (Time: 1664.4s)\n",
      "\n",
      "Processing k=7 for RAG-Sequence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (k=7): 100%|██████████| 254/254 [45:03<00:00, 10.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ Score (k=7): 36.37% (Time: 2703.7s)\n",
      "\n",
      "Processing k=10 for RAG-Sequence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (k=10): 100%|██████████| 254/254 [1:15:39<00:00, 17.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ Score (k=10): 37.01% (Time: 4539.3s)\n",
      "\n",
      "Processing k=20 for RAG-Sequence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (k=20): 100%|██████████| 254/254 [3:34:10<00:00, 50.59s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ Score (k=20): 37.45% (Time: 12850.4s)\n",
      "\n",
      "Processing k=30 for RAG-Sequence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (k=30):   0%|          | 0/254 [00:36<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.54 GiB. GPU 0 has a total capacity of 14.57 GiB of which 3.14 GiB is free. Process 2578282 has 11.43 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m rag_seq_mgr.set_n_docs(k)\n\u001b[32m     13\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m predictions = \u001b[43mrag_seq_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m score = calculate_em(predictions, answers)\n\u001b[32m     17\u001b[39m results[\u001b[33m\"\u001b[39m\u001b[33mrag-sequence\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m\"\u001b[39m].append(score)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mLocalRAGModelManager.generate_answers\u001b[39m\u001b[34m(self, questions, batch_size)\u001b[39m\n\u001b[32m     33\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(\n\u001b[32m     34\u001b[39m         batch_questions,\n\u001b[32m     35\u001b[39m         return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     36\u001b[39m         padding=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     37\u001b[39m         truncation=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     38\u001b[39m     ).to(device)\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m         generated_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m            \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     all_answers.extend(\u001b[38;5;28mself\u001b[39m.tokenizer.batch_decode(generated_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m all_answers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/models/rag/modeling_rag.py:1020\u001b[39m, in \u001b[36mRagSequenceForGeneration.generate\u001b[39m\u001b[34m(self, input_ids, attention_mask, context_input_ids, context_attention_mask, doc_scores, do_deduplication, num_return_sequences, num_beams, n_docs, **model_kwargs)\u001b[39m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1019\u001b[39m     new_input_ids = input_ids[index : index + \u001b[32m1\u001b[39m].repeat(num_candidates, \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_bos_score\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# input_ids is None, need context_input_ids/mask and doc_scores\u001b[39;00m\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m context_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[32m   1023\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMake sure that `context_attention_mask` are passed, if no `input_ids` is set. Alternatively, you\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1024\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m can set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1025\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/models/rag/modeling_rag.py:844\u001b[39m, in \u001b[36mRagSequenceForGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, exclude_bos_score, reduce_loss, labels, n_docs, **kwargs)\u001b[39m\n\u001b[32m    841\u001b[39m         decoder_input_ids = labels\n\u001b[32m    842\u001b[39m     use_cache = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoc_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoc_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_retrieved\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_retrieved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    861\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/models/rag/modeling_rag.py:680\u001b[39m, in \u001b[36mRagModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decoder_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    678\u001b[39m     decoder_attention_mask = decoder_attention_mask.repeat_interleave(n_docs, dim=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m gen_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_to_retrieve:\n\u001b[32m    693\u001b[39m     question_encoder_last_hidden_state = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:1577\u001b[39m, in \u001b[36mBartForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1572\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1573\u001b[39m         decoder_input_ids = shift_tokens_right(\n\u001b[32m   1574\u001b[39m             labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m   1575\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1577\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1578\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1579\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1580\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1581\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1582\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1583\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1584\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1585\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1586\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1589\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1591\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1593\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1595\u001b[39m lm_logits = \u001b[38;5;28mself\u001b[39m.lm_head(outputs[\u001b[32m0\u001b[39m])\n\u001b[32m   1596\u001b[39m lm_logits = lm_logits + \u001b[38;5;28mself\u001b[39m.final_logits_bias.to(lm_logits.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:1445\u001b[39m, in \u001b[36mBartModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1442\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1445\u001b[39m     encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1453\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[32m   1455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:1074\u001b[39m, in \u001b[36mBartEncoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1066\u001b[39m         layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1067\u001b[39m             encoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m   1068\u001b[39m             hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1071\u001b[39m             output_attentions,\n\u001b[32m   1072\u001b[39m         )\n\u001b[32m   1073\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1074\u001b[39m         layer_outputs = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m     hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1083\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:537\u001b[39m, in \u001b[36mBartEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    526\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    534\u001b[39m \u001b[33;03m        returned tensors for more detail.\u001b[39;00m\n\u001b[32m    535\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    536\u001b[39m residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m hidden_states, attn_weights, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    543\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m    544\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:231\u001b[39m, in \u001b[36mBartAttention.forward\u001b[39m\u001b[34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[39m\n\u001b[32m    228\u001b[39m value_states = value_states.reshape(*proj_shape)\n\u001b[32m    230\u001b[39m src_len = key_states.size(\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m attn_weights = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attn_weights.size() != (bsz * \u001b[38;5;28mself\u001b[39m.num_heads, tgt_len, src_len):\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    235\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttention weights should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m.num_heads,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39msrc_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but is\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    236\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_weights.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    237\u001b[39m     )\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 3.54 GiB. GPU 0 has a total capacity of 14.57 GiB of which 3.14 GiB is free. Process 2578282 has 11.43 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Evaluate Fine-Tuned RAG-SEQUENCE\n",
    "if PATH_TO_SEQUENCE_CHECKPOINT:\n",
    "    print(f\"\\n=== Evaluating Fine-Tuned RAG-Sequence ===\")\n",
    "    rag_seq_mgr = LocalRAGModelManager(\n",
    "        model_name=PATH_TO_SEQUENCE_CHECKPOINT,\n",
    "        rag_type=\"sequence\"\n",
    "    )\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        print(f\"\\nProcessing k={k} for RAG-Sequence...\")\n",
    "        rag_seq_mgr.set_n_docs(k)\n",
    "\n",
    "        start_time = time.time()\n",
    "        predictions = rag_seq_mgr.generate_answers(questions, batch_size=BATCH_SIZE)\n",
    "        score = calculate_em(predictions, answers)\n",
    "\n",
    "        results[\"rag-sequence\"][\"scores\"].append(score)\n",
    "        with open(\"benchmark_wq_k_results.json\", \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "        print(f\"WQ Score (k={k}): {score:.2f}% (Time: {time.time()-start_time:.1f}s)\")\n",
    "\n",
    "    del rag_seq_mgr\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipping Sequence Model (No path provided)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd70d01-2de8-47fc-b399-61cb5d3b61b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (6,) and (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Plot RAG-Sequence (falls Ergebnisse da sind)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results[\u001b[33m\"\u001b[39m\u001b[33mrag-sequence\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrag-sequence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mk\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrag-sequence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscores\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmarker\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFine-Tuned RAG-Sequence\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlinestyle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m--\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgreen\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m plt.title(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mRAG Performance on WebQuestions (Fine-Tuned) vs Retrieved Docs (k)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m(Samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(questions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     26\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mNumber of Retrieved Documents (k)\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/matplotlib/pyplot.py:3838\u001b[39m, in \u001b[36mplot\u001b[39m\u001b[34m(scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   3830\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.plot)\n\u001b[32m   3831\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot\u001b[39m(\n\u001b[32m   3832\u001b[39m     *args: \u001b[38;5;28mfloat\u001b[39m | ArrayLike | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3836\u001b[39m     **kwargs,\n\u001b[32m   3837\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[32m-> \u001b[39m\u001b[32m3838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3839\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3842\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3843\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3844\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1777\u001b[39m, in \u001b[36mAxes.plot\u001b[39m\u001b[34m(self, scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[33;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[32m   1536\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1774\u001b[39m \u001b[33;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1776\u001b[39m kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n\u001b[32m-> \u001b[39m\u001b[32m1777\u001b[39m lines = [*\u001b[38;5;28mself\u001b[39m._get_lines(\u001b[38;5;28mself\u001b[39m, *args, data=data, **kwargs)]\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[32m   1779\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_line(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/matplotlib/axes/_base.py:297\u001b[39m, in \u001b[36m_process_plot_var_args.__call__\u001b[39m\u001b[34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m     this += args[\u001b[32m0\u001b[39m],\n\u001b[32m    296\u001b[39m     args = args[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/matplotlib/axes/_base.py:494\u001b[39m, in \u001b[36m_process_plot_var_args._plot_args\u001b[39m\u001b[34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[39m\n\u001b[32m    491\u001b[39m     axes.yaxis.update_units(y)\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.shape[\u001b[32m0\u001b[39m] != y.shape[\u001b[32m0\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y must have same first dimension, but \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.ndim > \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y.ndim > \u001b[32m2\u001b[39m:\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y can be no greater than 2D, but have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    498\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: x and y must have same first dimension, but have shapes (6,) and (5,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAAH5CAYAAABDDuXVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQA5JREFUeJzt3Xl0VPX9//HXEEhAwgTCmpAgCIoiBLQIDBJFTLS0P8DSiEgL1C9uFC248AUpVPutEmpEofUnUrU9tlrjVwktPypRkISigiRGLEXKIktYElCUJIDZJvf3x3VCQraZbPfOzPNxTs7cuXNn8p7jOPDis7wdhmEYAgAAAIAg08bqAgAAAADACoQhAAAAAEGJMAQAAAAgKBGGAAAAAAQlwhAAAACAoEQYAgAAABCUCEMAAAAAglJbqwtoLhUVFTpx4oQ6deokh8NhdTkAAAAALGIYhoqKihQdHa02beoe/wmYMHTixAnFxsZaXQYAAAAAmzh69KhiYmLqfDxgwlCnTp0kmW/Y6XRaXA0AAAAAqxQWFio2NrYyI9QlYMKQZ2qc0+kkDAEAAABocPkMGygAAAAACEqEIQAAAABBiTAEAAAAICgRhgAAAAAEJcIQAAAAgKBEGAIAAAAQlAhDAAAAAIISYQgAAABAUCIMAQAAAAhKhCEAAAAAQYkwBAAAACAoEYYAAAAABCXCEAAAAICg1NbqAgAAAAD4L7db2rpVysuToqKk+HgpJMTqqrxDGAIAAADQKGlp0ty50rFjF87FxEgrV0qTJ1tXl7eYJgcAAADAZ2lpUlJS9SAkScePm+fT0qypyxeEIQAAAAA+cbvNESHDqPmY59y8eeZ1dkYYAgAAAOCTrVtrjghVZRjS0aPmdXbGmiEAAAAADSoqkrKypO3bpTVrvHtOXl7L1tRUhCEAAAAA1VRUSHv3msFn+3Zp2zZp927zvC+iolqmvuZCGAIAAACC3NdfSzt2mKFn+3bp44+lgoKa1116qTRqlDRihPTb30pffln7uiGHw9xVLj6+5WtvCsIQAAAAEETKy81RHk/w2b7dHAW6WIcO0nXXmeHH81N1pKdvX3PXOIejeiByOMzbFSvs32+IMAQAAAAEsJMnL4Se7dvNdT/nztW87vLLzcDjcpm3gwdL7drV/bqTJ0tvv117n6EVK/yjzxBhCAAAAAgQpaXSzp3Vw8+hQzWv69RJGjnyQvAZMULq1s333zd5sjRpkrlrXF6eOXIUH2//ESEPn8LQqlWrtGrVKh0+fFiSdPXVV+tXv/qVxo8fr7KyMi1evFjvvPOODh48qIiICCUkJGjZsmWKjo6u93VXrFihVatWKTc3V926dVNSUpKSk5PVvn37Rr8xAAAAINAdPVo9+HzyiVRSUv0ah0O6+urq092uvLL5AktIiDR2bPO8VmvzKQzFxMRo2bJlGjBggCTp1Vdf1aRJk/Tpp58qJiZGOTk5WrJkiYYOHapvvvlG8+bN08SJE5WdnV3na77++utauHCh/vjHP2r06NHat2+ffvazn0mSnnvuuca/MwAAACCAfPutGXaqhp/jx2te17Vr9eBz3XVSRETr1+sPHIZR2/4P3ouMjFRKSopmzZpV47GsrCyNGDFCR44cUZ8+fWp9/gMPPKA9e/bo/fffrzz3yCOPaMeOHdrqQ5emwsJCRUREqKCgQE6n0/c3AgAAANiEYUgHD1bf2vqzz8zND6oKCZGGDq0efgYMuLCJQbDyNhs0es2Q2+3WW2+9pXPnzsnlctV6TUFBgRwOhzp37lzn64wZM0avvfaaduzYoREjRujgwYN65513NHPmzHp/f0lJiUqqjAEWFhY26n0AAAAAVqva0NSzy9tXX9W8rlevC+t8Ro2Svvc9qWPH1q83UPgchnbt2iWXy6Xi4mKFh4dr7dq1GjRoUI3riouLtXDhQk2bNq3eNDZ16lR9+eWXGjNmjAzDUHl5uWbPnq2FCxfWW0dycrJ+/etf+1o+AAAAYClPQ9OqW1v/+981+/WEhkrXXnsh+LhcUmwsoz7NyedpcqWlpcrNzdWZM2e0Zs0avfzyy9qyZUu1QFRWVqbbb79dubm5yszMrDcMZWZmaurUqXryySc1cuRIHThwQHPnztU999yjJUuW1Pm82kaGYmNjmSYHAAAAW/n6a7OJqSf4NNTQ1BN8hg2TwsJavdyA4O00uSavGUpISFD//v21evVqSWYQmjJlig4ePKjNmzera9eu9T4/Pj5eo0aNUkpKSuW51157Tffee6/Onj2rNm3aeFUHa4YAAABgtfJyc5Sn6iYH9TU09Ux5GzmyekNTNE2LrxnyMAyjcoTGE4T279+vjIyMBoOQJJ0/f75G4AkJCZFhGGpiTgMAAABalC8NTauu9RkyRGpLx0/L+fSfYNGiRRo/frxiY2NVVFSk1NRUZWZmKj09XeXl5UpKSlJOTo7Wr18vt9ut/Px8SeaOc6GhoZKkGTNmqHfv3kpOTpYkTZgwQc8++6yuueaaymlyS5Ys0cSJExXiL92aAAAAEPAubmi6bZv0XfvNapxOc6THE3xGjjS3u4b9+BSGTp48qenTpysvL08RERGKi4tTenq6EhMTdfjwYa1bt06SNGzYsGrPy8jI0NjvOjHl5uZWGwlavHixHA6HFi9erOPHj6t79+6aMGGCnnrqqaa9MwAAAKCRDEM6dqx68MnJaf2GpmhZTV4zZBesGQIAAEBjeRqaVt3h7cSJmtdd3NB0xAhzJAj20mprhgAAAAB/UrWhqSf8eNPQ1OWS+vdna+tAQhgCAABAQCsqknbsqL7RQUMNTV0us6HpJZe0fr1oPYQhAAAABIyKCuk//6kefBpqaOoJQDQ0DT6EIQAAAPgtXxqaVt3amoamkAhDAAAA8BPeNjS95BKzoWnVra1paIraEIYAAABgS1Ubmm7bZjY0PX++5nVXXFF9hzcamsJbfEwAAABguaoNTT07vNHQFC2NMAQAAIBW5WloWrWnjzcNTV0us6FpmzbW1I3AQxgCAABAizp/3mxoWnWtT0MNTV0uc90PDU3RkghDAAAAaDaGIX3xRfXg01BDU88ubzQ0RWsjDAEAAKDRCgvNjQ0aamgaFVV9a2samsIOCEMAAADwii8NTb/3veo7vNHQFHZEGAIAAECtqjY03bZN2rGj9oamfftWDz40NIW/IAwBAACgWkNTzy5v+/bVvI6GpggkhCEAAIAglJ9vjvp4go83DU1dLmnwYBqaInDwUQYAAAhwpaXSp59WX+vTUENTl0saMYKGpghshCEAAIAAYhjS0aPVg09DDU09u7zR0BTBhjAEAADgx3xpaFp1a2samgKEIQAAAL9xcUPTbdvMhqZud/XrQkLMHd2q7vBGQ1OgJsIQAACATVVtaOrZ6OD06ZrX0dAUaBzCEAAAgA1UbWjqCT67d9PQFGhJhCEAAAALeBqaeoLPxx+bI0EXq9rQ1OWShg6loSnQXAhDAAAALczT0NQTfLxtaDpqlNSrV+vXCwQLwhAAAEAzy8+vvrtbQw1NPet9aGgKtC7+dwMAAGiCkhJp507vG5p6gg8NTQHrEYYAAAC85EtD08GDq093o6EpYD+EIQAAgDpc3NB02zYpL6/mdd26VQ8+NDQF/ANhCAAAQNUbmno2OqChKRDYCEMAACAoeRqaVt3hjYamQHAhDAEAgIDnaWhaNfg01NDUE4BiYhj1AQIVYQgAAASc06fNJqae4NNQQ1NP8KGhKRBcCEMAAMCvlZdLu3ZV3+GtvoamnuAzciQNTYFgRxgCAAB+xZeGplXX+tDQFMDF+EoAAAC2dXFD023bpCNHal4XEWGO9HiCDw1NAXiDMAQAAGyhakNTz0YHOTlSaWn162hoCqC5EIYAAIAlPA1Nq+7wRkNTAK2JMAQAAHzmdktbt5rhJSpKio83m5HWxdPQtGrwqa2hadu25o5unuDjckmXXcbW1gBaBmEIAAD4JC1NmjtXOnbswrmYGGnlSmnyZPN+YaG0Y0f1jQ4aamjqcknXXktDUwCthzAEAAC8lpYmJSXVbFZ6/Lj04x9LN98snTxZf0PTqju80dAUgJUIQwAAoE6GIRUVmaM6p05J991XM+R4rpOk99+/cK5fv+prfWhoCsBuCEMAAASJ4mIz1Jw+LX39de3HtT1WXu7b7/nNb6S776ahKQD7IwwBAOBnysulb75pOMRc/FhtjUm91b691KGD+Xsb0r8/QQiAfyAMAQBgkapT0LwdpTl9WjpzpvG/MyREiow0f7p2vfBT9X5tj11yiZSZKd10U8O/Iyqq8fUBQGsiDAEA0AzqmoJW34hNY6agVeV01h1o6go4Tmfjm5PGx5sbHhw/Xvu6IYfDfDw+vvHvCQBaE2EIAIAqvJmCVluoaeoUNG8CTdX7XbpI7do13/v2RkiIuX12UpIZfKoGIs+OcCtW1N9vCADshDAEAAhIvk5B89xvjilo3kw7q3q/Q4dme9stbvJk6e23a+8ztGLFhT5DAOAPCEMAANvzdgpa1ftNnYIWEeH7upqmTEHzJ5MnS5MmSVu3Snl55hqh+HhGhAD4H8IQAKDVNGYK2unT0rffNv53Vp2C5u26GiumoPmbkBBp7FirqwCApiEMAQB8ZhhSYaH3vWpacgpaQ/f9aQoaAKB1EYYAIMhVnYLmbTPO5piC5uu6GqfzwiJ9AACaA2EIAAKEZwqaL1s7N3UKWocOvq+riYyU2vKnDwDABvjjCABsprYpaN4EnOaagubtuhqmoAEA/B1hCABaUH1T0Oq63xJT0Bq6zxQ0AEAwIgwBgBe8nYJ28f2mTkHzdV1Nly5MQQMAwFv8kQkgqDRmCtrp01JBQeN/58VT0LwNOExBAwCgZRGGAHjF7bZfg8Vvv/V9a+fmmoLmy7oapqABAGBPhCEADUpLk+bOlY4du3AuJkZaudLsRN9UVaegebu1c0tMQWvoPlPQAAAILPyxDqBeaWlSUpI5vayq48fN82+/fSEQeaag+RJommMKmq/raiIjmYIGAAAIQwDq4XabI0IXByHpwrk775T69TNHdppzCpq3AYcpaAAAoLEIQwDqtHVr9alxtSktlfburX7OMwXNl3U1TEEDAACtjb96AKjVV1+Za4K88ctfSlOmMAUNAAD4F8IQgGpOnpSWL5deeEE6d8675yQkSHFxLVsXAABAcyMMAZBkbojw9NPSH/4gFReb54YNk44eNdcC1bZuyOEwd5WLj2/VUgEAAJpFG6sLAGCtI0ek2bOlyy6Tfvc7MwiNHCmtXy/l5JjhSKq5SYHn/ooV1vcbAgAAaAzCEBCkDhyQZs2SBgyQXnzR3AjhhhukjRulbdukH/7QDDyTJ5vbZ/fuXf35MTHVt9UGAADwN0yTA4LMnj3SU09Jb7whVVSY5xISpCVLzDBUm8mTpUmTzN3l8vKkqChzahwjQgAAwJ8RhoAg8a9/SU8+aY7meNb//PCH0uLF0qhRDT8/JEQaO7ZFSwQAAGhVPk2TW7VqleLi4uR0OuV0OuVyubRhwwZJUllZmRYsWKAhQ4aoY8eOio6O1owZM3TixIkGX/fMmTOaM2eOoqKi1L59e1111VV65513GveOAFTzySfSbbdJQ4dKb71lBqEf/cg8v369d0EIAAAgEPk0MhQTE6Nly5ZpwIABkqRXX31VkyZN0qeffqqYmBjl5ORoyZIlGjp0qL755hvNmzdPEydOVHZ2dp2vWVpaqsTERPXo0UNvv/22YmJidPToUXXq1Klp7wwIctu2Sb/5jfTdv1fI4TB7Af3yl9KQIdbWBgAAYAcOw6htw1zvRUZGKiUlRbNmzarxWFZWlkaMGKEjR46oT58+tT7/xRdfVEpKiv7zn/+oXbt2Xv/ekpISlZSUVN4vLCxUbGysCgoK5HQ6fX8jQAAwDGnLFjMEbd5sngsJkaZNkxYtkq680tr6AAAAWkNhYaEiIiIazAaN3k3O7XYrNTVV586dk8vlqvWagoICORwOde7cuc7XWbdunVwul+bMmaOePXtq8ODBWrp0qdxud72/Pzk5WREREZU/sbGxjX0rgN8zDOm998wNEG66yQxCbdtKd98t7d0r/fnPBCEAAICL+TwytGvXLrlcLhUXFys8PFx//etf9YMf/KDGdcXFxRozZoyuvPJKvfbaa3W+3pVXXqnDhw/rJz/5iX7+859r//79mjNnjubOnatf/epXdT6PkSHADEH/+Ic5ErRjh3kuNNQMQQsWSHUMyAIAAAQ0b0eGfA5DpaWlys3N1ZkzZ7RmzRq9/PLL2rJliwYNGlR5TVlZmW6//Xbl5uYqMzOz3gKuuOIKFRcX69ChQwr5bp/eZ599VikpKcrLy/O6Lm/fMBAIKiqktWvN3eF27jTPdegg3XefNH++FB1taXkAAACW8jYb+Ly1dmhoaOUGCsOHD1dWVpZWrlyp1atXSzKD0JQpU3To0CFt3ry5wWASFRWldu3aVQYhSbrqqquUn5+v0tJShYaG+loiELDcbul//9fsE7R7t3kuPFyaM0d6+GGpRw9r6wMAAPAnjV4z5GEYRuV0NU8Q2r9/vzZt2qSuXbs2+Pzrr79eBw4cUIWn+6Okffv2KSoqiiAEfKesTHr1VWnQIHMzhN27pYgIs1Hq4cPSsmUEIQAAAF/5NDK0aNEijR8/XrGxsSoqKlJqaqoyMzOVnp6u8vJyJSUlKScnR+vXr5fb7VZ+fr4kc8c5T7CZMWOGevfureTkZEnS7Nmz9fvf/15z587Vgw8+qP3792vp0qX6xS9+0cxvFfA/paVmCEpOlg4dMs9FRkoPPSQ98IBUz94kAAAAaIBPYejkyZOaPn268vLyFBERobi4OKWnpysxMVGHDx/WunXrJEnDhg2r9ryMjAyN/a51fW5urtq0uTAgFRsbq/fee08PPfSQ4uLi1Lt3b82dO1cLFixo2jsD/FhxsfTyy9JvfysdO2ae69FDeuQRafZsiTZcAAAATdfkPkN2wQYKCATnzkmrV0spKdJ3A6uKipL++7+le++VLrnE2voAAAD8QYttoACg+RUWSi+8IC1fLn31lXmuTx9p4ULprruk9u2trQ8AACAQEYYAC33zjfS730krV5rHknTZZdKiRdL06WbPIAAAALQMwhBgga++kp57Tnr+eXNUSJIGDpR++UvpzjultvyfCQAA0OL4KxfQivLzzalwq1aZ64MkafBgafFiKSlJqtJuCwAAAC2MMAS0gmPHpKefll56ydwpTpKuvdbsEzRxotSmyR2/AAAA4CvCENCCPA1R//Qns2eQJI0aZYag8eMlh8PS8gAAAIIaYQhoAQcOSEuXSn/5i1Rebp674QYzBN18MyEIAADADghDQDPas0d66inpjTekigrzXEKCGYJuuMHa2gAAAFAdYQhoBp99Jj35pLRmjeRpY/zDH5obI4waZW1tAAAAqB1hCGiC7GzpN7+R1q27cO5HPzJD0LXXWlcXAAAAGkYYAhrho4/MEJSebt53OKQpU8w+QUOGWFsbAAAAvEMYArxkGFJmphmCMjLMcyEh0k9+Ii1aZDZNBQAAgP8gDAENMAzpvffMEPThh+a5du2kmTOlhQul/v2trQ8AAACNQxgC6mAY0vr1ZgjKyjLPhYZKd98tLVgg9eljbX0AAABoGsIQcJGKCiktzdwd7rPPzHMdOkj33SfNny9FR1tbHwAAAJoHYQhBxe2Wtm6V8vKkqCgpPt5c9+N57M03zT5Bn39ungsPl+bMkR5+WOrRw7q6AQAA0PwIQwgaaWnS3LnSsWMXzsXESMuXS+fPS0uXSvv3m+cjIqRf/MK8vmtXa+oFAABAyyIMISikpUlJSRcaonocOybdcceF+5GR0kMPSQ88IHXu3KolAgAAoJURhhDw3G5zhOfiIFRVmzbmyNDPfy516tR6tQEAAMA6bawuAGhpW7dWnxpXm4oKaeRIghAAAEAwIQwh4OXlNe91AAAACAyEIQS8qKjmvQ4AAACBgTCEgBcfb+4a53DU/rjDIcXGmtcBAAAgeBCGEPBCQqSVK2t/zBOQVqy40G8IAAAAwYEwhKAwebL05z/XPB8TI739tvk4AAAAggtbayNoeHaK691bSkkx1wjFxzMiBAAAEKwIQwga771n3t52m3TnnZaWAgAAABtgmhyChicM3XKLtXUAAADAHghDCAoHD0oHDkht20pjx1pdDQAAAOyAMISgsHGjeTtqlOR0WlsLAAAA7IEwhKDAFDkAAABcjDCEgFdeLr3/vnlMGAIAAIAHYQgBLytLKiiQOneWhg+3uhoAAADYBWEIAc8zRS4hgZ5CAAAAuIAwhIDHeiEAAADUhjCEgHbmjPTxx+ZxYqKlpQAAAMBmCEMIaBkZktstXXGF1Lev1dUAAADATghDCGhMkQMAAEBdCEMIaIQhAAAA1IUwhID1xRfSwYNS27bS2LFWVwMAAAC7IQwhYHlGhUaPljp1srYWAAAA2A9hCAGLKXIAAACoD2EIAamsTNq82TwmDAEAAKA2hCEEpB07pMJCKTJSuvZaq6sBAACAHRGGEJA8U+QSEqSQEGtrAQAAgD0RhhCQWC8EAACAhhCGEHC++cacJidJiYnW1gIAAAD7Igwh4GzeLFVUSFdeKfXpY3U1AAAAsCvCEAIOU+QAAADgDcIQAophSO++ax4ThgAAAFAfwhACyoED0pEjUrt20o03Wl0NAAAA7IwwhIDimSJ3/fVSeLi1tQAAAMDeCEMIKKwXAgAAgLcIQwgYZWXmTnISYQgAAAANIwwhYGzfLp09K3XtKl1zjdXVAAAAwO4IQwgYGzeat4mJUhs+2QAAAGgAf2VEwGC9EAAAAHxBGEJA+PprKSvLPE5MtLYWAAAA+AfCEALC5s1SRYU0aJAUE2N1NQAAAPAHhCEEBKbIAQAAwFeEIfg9wyAMAQAAwHeEIfi9/fulI0ek0FDphhusrgYAAAD+gjAEv+cZFRozRurY0dpaAAAA4D8IQ/B7TJEDAABAYxCG4NdKS6WMDPOYMAQAAABfEIbg17Zvl86elbp3l4YOtboaAAAA+BOfwtCqVasUFxcnp9Mpp9Mpl8ulDRs2SJLKysq0YMECDRkyRB07dlR0dLRmzJihEydOeP36qampcjgcuu2223x6EwhenilyiYlSG6I9AAAAfODTXx9jYmK0bNkyZWdnKzs7W+PGjdOkSZO0e/dunT9/Xjk5OVqyZIlycnKUlpamffv2aeLEiV699pEjR/Too48qPj6+UW8EwYn1QgAAAGgsh2EYRlNeIDIyUikpKZo1a1aNx7KysjRixAgdOXJEffr0qfM13G63brzxRt11113aunWrzpw5o7/97W8+1VFYWKiIiAgVFBTI6XT6+jbgh06fNqfHGYZ0/LgUHW11RQAAALADb7NBoycWud1upaam6ty5c3K5XLVeU1BQIIfDoc6dO9f7Wv/zP/+j7t271xqo6lJSUqLCwsJqPwgu779vBqHBgwlCAAAA8F1bX5+wa9cuuVwuFRcXKzw8XGvXrtWgQYNqXFdcXKyFCxdq2rRp9aaxDz/8UK+88op27tzpUx3Jycn69a9/7Wv5CCBMkQMAAEBT+DwyNHDgQO3cuVPbt2/X7NmzNXPmTH3++efVrikrK9PUqVNVUVGhF154oc7XKioq0k9/+lO99NJL6tatm091PPbYYyooKKj8OXr0qK9vBX7MMAhDAAAAaJomrxlKSEhQ//79tXr1aklmEJoyZYoOHjyozZs3q2vXrnU+d+fOnbrmmmsUEhJSea6iokKS1KZNG+3du1f9+/f3qg7WDAWX//xHuuoqKSxM+vpr6ZJLrK4IAAAAduFtNvB5mtzFDMNQSUmJpAtBaP/+/crIyKg3CEnSlVdeqV27dlU7t3jxYhUVFWnlypWKjY1tankIUJ5Rofh4ghAAAAAax6cwtGjRIo0fP16xsbEqKipSamqqMjMzlZ6ervLyciUlJSknJ0fr16+X2+1Wfn6+JHPHudDQUEnSjBkz1Lt3byUnJ6t9+/YaPHhwtd/h2Wzh4vNAVUyRAwAAQFP5FIZOnjyp6dOnKy8vTxEREYqLi1N6eroSExN1+PBhrVu3TpI0bNiwas/LyMjQ2LFjJUm5ublqQ3dMNEFJiZSRYR4ThgAAANBYTV4zZBesGQoemZnSTTdJPXtKJ05IZGsAAABU1eJ9hgCreKbIJSYShAAAANB4/FUSfof1QgAAAGgOhCH4lS+/lHJyzOOEBGtrAQAAgH8jDMGvvP++2XB1yBApKsrqagAAAODPCEPwK0yRAwAAQHMhDMFvGAZhCAAAAM2HMAS/sWePdPy4FBYmxcdbXQ0AAAD8HWEIfsMzKnTDDVKHDtbWAgAAAP9HGILfYIocAAAAmhNhCH6hpETKzDSPCUMAAABoDoQh+IUPP5S+/Vbq2dPcVhsAAABoKsIQ/ELVKXIOh7W1AAAAIDAQhuAXWC8EAACA5kYYgu2dOiV9+ql5nJBgbS0AAAAIHIQh2N6mTebt0KFSr17W1gIAAIDAQRiC7TFFDgAAAC2BMARbMwzCEAAAAFoGYQi2tnu3lJcntW8vjRljdTUAAAAIJIQh2JpnVOjGG81ABAAAADQXwhBsjSlyAAAAaCmEIdhWcbG0ZYt5TBgCAABAcyMMwbY++MAMRFFR0tVXW10NAAAAAg1hCLZVdYqcw2FtLQAAAAg8hCHYFuuFAAAA0JIIQ7Cl/Hzps8/M44QEa2sBAABAYCIMwZY2bTJvr7lG6tHD2loAAAAQmAhDsKWNG81bpsgBAACgpRCGYDuGwXohAAAAtDzCEGzn3/821wx16CBdf73V1QAAACBQEYZgO55RobFjpbAwS0sBAABAACMMwXaYIgcAAIDWQBiCrXz7rfTPf5rHhCEAAAC0JMIQbOWDD6TiYql3b+mqq6yuBgAAAIGMMARbqTpFzuGwthYAAAAENsIQbIX1QgAAAGgthCHYRl6e9K9/mSNCCQlWVwMAAIBARxiCbWzaZN5ee63UrZu1tQAAACDwEYZgG0yRAwAAQGsiDMEWKiqkjRvNY8IQAAAAWgNhCLawa5d08qTUsaPkclldDQAAAIIBYQi24JkiN3asFBZmaSkAAAAIEoQh2ALrhQAAANDaCEOw3Pnz0tat5jFhCAAAAK2FMATLbd0qlZRIsbHSwIFWVwMAAIBgQRiC5apOkXM4rK0FAAAAwYMwBMuxXggAAABWIAzBUidOSP/+tzkidPPNVlcDAACAYEIYgqU8jVaHD5e6drW2FgAAAAQXwhAsxRQ5AAAAWIUwBMtUVFwYGSIMAQAAoLURhmCZzz6TvvxSCg+XRo2yuhoAAAAEG8IQLOOZInfTTVJoqLW1AAAAIPgQhmAZ1gsBAADASoQhWOLcOemDD8xjwhAAAACsQBiCJf75T6m0VOrTR7r8cqurAQAAQDAiDMESVafIORzW1gIAAIDgRBiCJVgvBAAAAKsRhtDqjh2TPv/cHBG6+WarqwEAAECwIgyh1XkarV53nRQZaW0tAAAACF6EIbQ6psgBAADADghDaFUVFRdGhghDAAAAsBJhCK3q00+l06el8HBp1CirqwEAAEAwIwyhVXmmyI0bJ7VrZ20tAAAACG6EIbQq1gsBAADALghDaDVnz0offmgeE4YAAABgNcIQWs2WLVJZmdS3rzRggNXVAAAAINgRhtBqqk6RczisrQUAAADwKQytWrVKcXFxcjqdcjqdcrlc2rBhgySprKxMCxYs0JAhQ9SxY0dFR0drxowZOnHiRL2v+dJLLyk+Pl5dunRRly5dlJCQoB07djT+HcG2WC8EAAAAO/EpDMXExGjZsmXKzs5Wdna2xo0bp0mTJmn37t06f/68cnJytGTJEuXk5CgtLU379u3TxIkT633NzMxM3XnnncrIyNC2bdvUp08f3XLLLTp+/HiT3hjsJTdX+s9/pDZtzJ3kAAAAAKs5DMMwmvICkZGRSklJ0axZs2o8lpWVpREjRujIkSPq06ePV6/ndrvVpUsXPf/885oxY4bXdRQWFioiIkIFBQVyOp1ePw+t45VXpLvvNnsLbdtmdTUAAAAIZN5mg7aN/QVut1tvvfWWzp07J5fLVes1BQUFcjgc6ty5s9eve/78eZWVlSkyMrLe60pKSlRSUlJ5v7Cw0OvfgdbHFDkAAADYjc8bKOzatUvh4eEKCwvT/fffr7Vr12rQoEE1risuLtbChQs1bdo0n0ZqFi5cqN69eyshIaHe65KTkxUREVH5Exsb6+tbQStxu6VNm8xjwhAAAADswudpcqWlpcrNzdWZM2e0Zs0avfzyy9qyZUu1QFRWVqbbb79dubm5yszM9DoMPf3001q2bJkyMzMVFxdX77W1jQzFxsYyTc6GsrKkESMkp1P66iupXTurKwIAAEAga7FpcqGhoRrwXZOY4cOHKysrSytXrtTq1aslmUFoypQpOnTokDZv3ux1MHnmmWe0dOlSbdq0qcEgJElhYWEKCwvztXxYwDNFbtw4ghAAAADso9FrhjwMw6gcofEEof379ysjI0Ndu3b16jVSUlL05JNP6t1339Xw4cObWhJsZuNG85YpcgAAALATn8LQokWLNH78eMXGxqqoqEipqanKzMxUenq6ysvLlZSUpJycHK1fv15ut1v5+fmSzB3nQkNDJUkzZsxQ7969lZycLMmcGrdkyRL99a9/Vd++fSufEx4ervDw8OZ8r7BAUZH00UfmMWEIAAAAduJTGDp58qSmT5+uvLw8RUREKC4uTunp6UpMTNThw4e1bt06SdKwYcOqPS8jI0Njx46VJOXm5qpNmwv7NrzwwgsqLS1VUlJStec8/vjjeuKJJ3x/R7CVLVuksjLpssuk/v2trgYAAAC4wKcw9Morr9T5WN++feXNXgyZmZnV7h8+fNiXEuBn2FIbAAAAduXz1tqALwhDAAAAsCvCEFrMkSPS3r1SSIh0001WVwMAAABURxhCi/HsIjdypNS5s6WlAAAAADUQhtBimCIHAAAAOyMMoUW43dKmTeYxYQgAAAB2RBhCi/jkE+mbb6SICOm666yuBgAAAKiJMIQW4Zkid/PNUlufNnAHAAAAWgdhCC2C9UIAAACwO8IQml1hobRtm3lMGAIAAIBdEYbQ7DIzpfJyacAAqV8/q6sBAAAAakcYQrNjihwAAAD8AWEIzY4wBAAAAH9AGEKzOnRI2r9fCgmRbrrJ6moAAACAuhGG0Kw2bjRvXS7J6bS2FgAAAKA+hCE0K6bIAQAAwF8QhtBsysul9983jwlDAAAAsDvCEJpNdrZ05ozUubM0fLjV1QAAAAD1Iwyh2XimyCUkmBsoAAAAAHZGGEKzYb0QAAAA/AlhCM2ioEDavt08Tky0thYAAADAG4QhNIuMDMntlq64Qurb1+pqAAAAgIYRhtAsmCIHAAAAf0MYQrMgDAEAAMDfEIbQZF98Yf60bSuNHWt1NQAAAIB3CENoso0bzdvRo6VOnaytBQAAAPAWYQhNxhQ5AAAA+CPCEJqkvFx6/33zmDAEAAAAf0IYQpPs2CEVFkpdukjXXmt1NQAAAID3CENoEs8UuYQEKSTE2loAAAAAXxCG0CSsFwIAAIC/Igyh0c6ckT7+2DxOTLS0FAAAAMBnhCE02ubNUkWFNHCgdOmlVlcDAAAA+IYwhEZjihwAAAD8GWEIjWIY0rvvmseEIQAAAPgjwhAa5YsvpMOHpXbtpLFjra4GAAAA8B1hCI3imSI3erQUHm5tLQAAAEBjEIbQKKwXAgAAgL8jDMFnZWXmTnISYQgAAAD+izAEn338sVRUJHXtKl1zjdXVAAAAAI1DGILPPFPkEhKkkBBrawEAAAAaizAEn7FeCAAAAIGAMASffP21lJVlHicmWlsLAAAA0BSEIfhk82apokK66iopNtbqagAAAIDGIwzBJ0yRAwAAQKAgDMFrhkEYAgAAQOAgDMFrBw5IR45I7dpJN95odTUAAABA0xCG4DXPqNCYMVLHjtbWAgAAADQVYQheY4ocAAAAAglhCF4pKzN3kpMIQwAAAAgMhCF4Zft26exZqVs3adgwq6sBAAAAmo4wBK94psglJkpt+NQAAAAgALS1ugDYm9stbd0qvfGGeT8hwdp6AAAAgObCv/GjTmlpUt++0k03SV98YZ5bvNg8DwAAAPg7whBqlZYmJSVJx45VP5+fb54nEAEAAMDfEYZQg9stzZ0rGUbNxzzn5s0zrwMAAAD8FWEINWzdWnNEqCrDkI4eNa8DAAAA/BVhCDXk5TXvdQAAAIAdEYZQQ1RU814HAAAA2BFhCDXEx0sxMXU/7nBIsbHmdQAAAIC/IgyhhpAQaeXK2h9zOMzbFSvM6wAAAAB/RRhCrS67rPbzMTHS229Lkye3bj0AAABAc2trdQGwp+XLzdspU6TZs83NEqKizKlxjAgBAAAgEBCGUMPRo1Jqqnn83/8tfe971tYDAAAAtASmyaGGlSul8nLpppsIQgAAAAhchCFUU1Ag/eEP5vGjj1pbCwAAANCSCEOoZvVqqahIuvpqafx4q6sBAAAAWo5PYWjVqlWKi4uT0+mU0+mUy+XShg0bJEllZWVasGCBhgwZoo4dOyo6OlozZszQiRMnGnzdNWvWaNCgQQoLC9OgQYO0du3axr0bNElp6YUttR999MI22gAAAEAg8ikMxcTEaNmyZcrOzlZ2drbGjRunSZMmaffu3Tp//rxycnK0ZMkS5eTkKC0tTfv27dPEiRPrfc1t27bpjjvu0PTp0/XZZ59p+vTpmjJlij7++OMmvTH47o03pBMnzF3j7rzT6moAAACAluUwDMNoygtERkYqJSVFs2bNqvFYVlaWRowYoSNHjqhPnz61Pv+OO+5QYWFh5QiTJH3/+99Xly5d9MYbb3hdR2FhoSIiIlRQUCCn0+n7GwlyhiHFxUn//re0bJm0YIHVFQEAAACN4202aPSaIbfbrdTUVJ07d04ul6vWawoKCuRwONS5c+c6X2fbtm265ZZbqp279dZb9dFHH9X7+0tKSlRYWFjtB4337rtmEAoPl+67z+pqAAAAgJbncxjatWuXwsPDFRYWpvvvv19r167VoEGDalxXXFyshQsXatq0afWmsfz8fPXs2bPauZ49eyo/P7/eOpKTkxUREVH5Exsb6+tbQRUpKebtPfdI9WRXAAAAIGD4HIYGDhyonTt3avv27Zo9e7Zmzpypzz//vNo1ZWVlmjp1qioqKvTCCy80+JqOi1bqG4ZR49zFHnvsMRUUFFT+HD161Ne3gu/k5EibN0shIdK8eVZXAwAAALSOtr4+ITQ0VAMGDJAkDR8+XFlZWVq5cqVWr14tyQxCU6ZM0aFDh7R58+YG1+/06tWrxijQqVOnaowWXSwsLExhYWG+lo9aPPOMeTt1qlTH0i4AAAAg4DS5z5BhGCopKZF0IQjt379fmzZtUteuXRt8vsvl0saNG6ude++99zR69OimlgYvHDki/e//msc0WQUAAEAw8WlkaNGiRRo/frxiY2NVVFSk1NRUZWZmKj09XeXl5UpKSlJOTo7Wr18vt9tdOeITGRmp0NBQSdKMGTPUu3dvJScnS5Lmzp2rG264Qb/97W81adIk/f3vf9emTZv0wQcfNPNbRW2ee05yu6WEBGnYMKurAQAAAFqPT2Ho5MmTmj59uvLy8hQREaG4uDilp6crMTFRhw8f1rp16yRJwy76W3VGRobGjh0rScrNzVWbNhcGpEaPHq3U1FQtXrxYS5YsUf/+/fXmm29q5MiRTXtnaNA330gvv2wez59vbS0AAABAa2tynyG7oM+Q75KTpUWLzP5CO3dKDexZAQAAAPiFFu8zBP9WUiL97nfm8aOPEoQAAAAQfAhDQer116X8fKl3b+mOO6yuBgAAAGh9hKEgVFFxYTvtefOk7/a2AAAAAIIKYSgIbdgg7dkjOZ3SvfdaXQ0AAABgDcJQEEpJMW/vvdcMRAAAAEAwIgwFmawsacsWqW1bae5cq6sBAAAArEMYCjKetULTpkkxMdbWAgAAAFiJMBREDh6U3n7bPH7kEWtrAQAAAKxGGAoizz1n7iR3661mo1UAAAAgmBGGgsTp09If/2gez59vbS0AAACAHRCGgsSqVdL589KwYdK4cVZXAwAAAFiPMBQEioul3//ePJ4/X3I4rK0HAAAAsAPCUBD4y1+kU6ek2Fjp9tutrgYAAACwB8JQgKuokJYvN48fekhq187aegAAAAC7IAwFuPXrpb17pYgI6e67ra4GAAAAsA/CUIBLSTFv779f6tTJ2loAAAAAOyEMBbDt26UPPjCnxv3iF1ZXAwAAANgLYSiAPfOMefvTn0rR0dbWAgAAANgNYShAHTggpaWZx488Ym0tAAAAgB0RhgLUs89KhiH94AfS1VdbXQ0AAABgP4ShAPTll9Kf/mQez59vbS0AAACAXRGGAtALL0jFxdL3vifdeKPV1QAAAAD2RBgKMN9+Kz3/vHk8f77kcFhbDwAAAGBXhKEA8+qr0ldfSX37Sj/+sdXVAAAAAPZFGAogbre0fLl5/NBDUtu21tYDAAAA2BlhKICsW2duqd2li/Rf/2V1NQAAAIC9EYYCSEqKeTt7thQebm0tAAAAgN0RhgLEhx9K27ZJoaHSgw9aXQ0AAABgf4ShAPHMM+btjBlSr17W1gIAAAD4A8JQANi3T/r7383jhx+2thYAAADAXxCGAsDy5ZJhSBMmSFddZXU1AAAAgH8gDPm5U6fM3kKS2WQVAAAAgHcIQ37u+eelkhJpxAhpzBirqwEAAAD8B2HIj50/L/3f/2sez58vORzW1gMAAAD4E8KQH/vTn6Svv5Yuu0z60Y+srgYAAADwL4QhP+V2S88+ax4//LAUEmJtPQAAAIC/IQz5qbVrpYMHpa5dpbvusroaAAAAwP8QhvyQYUgpKebxz38uXXKJtfUAAAAA/ogw5Ie2bpV27JDat5ceeMDqagAAAAD/RBjyQ888Y97OnCn16GFtLQAAAIC/Igz5mT17pP/3/8xttB9+2OpqAAAAAP9FGPIzy5ebt5MmSVdcYW0tAAAAgD8jDPmR/HzpL38xj+fPt7YWAAAAwN8RhvzI738vlZZKLpc0erTV1QAAAAD+jTDkJ86elVatMo8ZFQIAAACajjDkJ/74R+mbb6TLL5cmTrS6GgAAAMD/EYb8QHm59Nxz5vHDD0shIdbWAwAAAAQCwpAfWLNGOnxY6t7d7C0EAAAAoOkIQzZnGFJKinn8wANShw7W1gMAAAAECsKQzWVmSp98Yoagn//c6moAAACAwEEYsrlnnjFv77pL6tbN2loAAACAQEIYsrHdu6V33pEcDumhh6yuBgAAAAgshCEb84wKTZ4sDRhgbS0AAABAoCEM2dSJE9Lrr5vHNFkFAAAAmh9hyKZ+9zuprEwaM0YaOdLqagAAAIDAQxiyoaIi6cUXzWNGhQAAAICWQRiyoZdflgoKpIEDpf/zf6yuBgAAAAhMhCGbKSuTnnvOPH7kEakN/4UAAACAFsFftW3mrbeko0elnj2l6dOtrgYAAAAIXIQhGzEMKSXFPH7wQal9e2vrAQAAAAIZYchG3n9f2rlTuuQS6f77ra4GAAAACGyEIRvxNFmdNUvq2tXaWgAAAIBARxiyiX/9S3r3XXPDhIcesroaAAAAIPARhmzCMyqUlCT162dtLQAAAEAwIAzZwLFj0htvmMc0WQUAAABaB2HIBlaulMrLpRtvlIYPt7oaAAAAIDgQhixWUCCtXm0eMyoEAAAAtB6fwtCqVasUFxcnp9Mpp9Mpl8ulDRs2VD6elpamW2+9Vd26dZPD4dDOnTu9et0VK1Zo4MCB6tChg2JjY/XQQw+puLjYpzfir156SSoqkgYNksaPt7oaAAAAIHj4FIZiYmK0bNkyZWdnKzs7W+PGjdOkSZO0e/duSdK5c+d0/fXXa9myZV6/5uuvv66FCxfq8ccf1549e/TKK6/ozTff1GOPPebbO/FDpaXSihXm8SOPmDvJAQAAAGgdbX25eMKECdXuP/XUU1q1apW2b9+uq6++WtOnT5ckHT582OvX3LZtm66//npNmzZNktS3b1/deeed2rFjhy+l+aU335SOH5eioqSf/MTqagAAAIDg0uixCLfbrdTUVJ07d04ul6vRBYwZM0affPJJZfg5ePCg3nnnHf3whz+s93klJSUqLCys9uNPDENKSTGPf/ELKSzM2noAAACAYOPTyJAk7dq1Sy6XS8XFxQoPD9fatWs1aNCgRhcwdepUffnllxozZowMw1B5eblmz56thQsX1vu85ORk/frXv27077Xae+9Ju3ZJHTtK991ndTUAAABA8PF5ZGjgwIHauXOntm/frtmzZ2vmzJn6/PPPG11AZmamnnrqKb3wwgvKyclRWlqa1q9fr9/85jf1Pu+xxx5TQUFB5c/Ro0cbXYMVPE1W77lH6tLF2loAAACAYOQwDMNoygskJCSof//+Wu3ZH1rmmqF+/frp008/1bBhw+p9fnx8vEaNGqUUz5wxSa+99pruvfdenT17Vm283FWgsLBQERERKigokNPpbNR7aS2ffipde60UEiJ98YV06aVWVwQAAAAEDm+zQZP3LzMMQyUlJY1+/vnz52sEnpCQEBmGoSbmNNvyjApNmUIQAgAAAKzi05qhRYsWafz48YqNjVVRUZFSU1OVmZmp9PR0SdLXX3+t3NxcnThxQpK0d+9eSVKvXr3Uq1cvSdKMGTPUu3dvJScnSzJ3qHv22Wd1zTXXaOTIkTpw4ICWLFmiiRMnKiQkpNneqF3k5pq7yEnSo49aWwsAAAAQzHwKQydPntT06dOVl5eniIgIxcXFKT09XYmJiZKkdevW6a677qq8furUqZKkxx9/XE888YQkKTc3t9pI0OLFi+VwOLR48WIdP35c3bt314QJE/TUU0819b3Z0ooVktstjRtnTpUDAAAAYI0mrxmyC39YM3TmjBQbK509K23YIH3/+1ZXBAAAAASeVlszBO+tXm0GocGDpVtvtboaAAAAILgRhlpJSYm0cqV5/OijksNhbT0AAABAsCMMtZI33pDy8qTevaU777S6GgAAAACEoVZgGBe20547VwoNtbYeAAAAAIShVrFhg7R7t9Spk3TvvVZXAwAAAEAiDLUKz6jQvfdKERHW1gIAAADARBhqYZ98ImVkSG3bmlPkAAAAANgDYaiFpaSYt1Onmj2GAAAAANgDYagFHT4svfWWefzoo5aWAgAAAOAihKEW9NxzUkWFlJgoDR1qdTUAAAAAqmprdQGBxu2Wtm6V9u+X/vAH89z8+dbWBAAAAKAmwlAzSkszN0k4duzCuXbtpMJC62oCAAAAUDumyTWTtDQpKal6EJKksjLp9tvNxwEAAADYB2GoGbjd5oiQYdR9zbx55nUAAAAA7IEw1Ay2bq05IlSVYUhHj5rXAQAAALAHwlAzyMtr3usAAAAAtDzCUDOIimre6wAAAAC0PMJQM4iPl2JiJIej9scdDik21rwOAAAAgD0QhppBSIi0cqV5fHEg8txfscK8DgAAAIA9EIaayeTJ0ttvS717Vz8fE2OenzzZmroAAAAA1I6mq81o8mRp0iRz17i8PHONUHw8I0IAAACAHRGGmllIiDR2rNVVAAAAAGgI0+QAAAAABCXCEAAAAICgRBgCAAAAEJQIQwAAAACCEmEIAAAAQFAiDAEAAAAISoQhAAAAAEGJMAQAAAAgKBGGAAAAAAQlwhAAAACAoEQYAgAAABCUCEMAAAAAghJhCAAAAEBQamt1Ac3FMAxJUmFhocWVAAAAALCSJxN4MkJdAiYMFRUVSZJiY2MtrgQAAACAHRQVFSkiIqLOxx1GQ3HJT1RUVOjEiRPq1KmTHA6H1eUElMLCQsXGxuro0aNyOp1WlwML8VmAxOcAF/BZgMTnACa7fQ4Mw1BRUZGio6PVpk3dK4MCZmSoTZs2iomJsbqMgOZ0Om3x4Yb1+CxA4nOAC/gsQOJzAJOdPgf1jQh5sIECAAAAgKBEGAIAAAAQlAhDaFBYWJgef/xxhYWFWV0KLMZnARKfA1zAZwESnwOY/PVzEDAbKAAAAACALxgZAgAAABCUCEMAAAAAghJhCAAAAEBQIgwBAAAACEqEIQAAAABBiTCEOj3xxBNyOBzVfnr16mV1WWhh//znPzVhwgRFR0fL4XDob3/7W7XHDcPQE088oejoaHXo0EFjx47V7t27rSkWLaqhz8LPfvazGt8Ro0aNsqZYtJjk5GRdd9116tSpk3r06KHbbrtNe/furXYN3wuBz5vPAd8JwWHVqlWKi4uT0+mU0+mUy+XShg0bKh/3t+8DwhDqdfXVVysvL6/yZ9euXVaXhBZ27tw5DR06VM8//3ytjz/99NN69tln9fzzzysrK0u9evVSYmKiioqKWrlStLSGPguS9P3vf7/ad8Q777zTihWiNWzZskVz5szR9u3btXHjRpWXl+uWW27RuXPnKq/heyHwefM5kPhOCAYxMTFatmyZsrOzlZ2drXHjxmnSpEmVgcfvvg8MoA6PP/64MXToUKvLgIUkGWvXrq28X1FRYfTq1ctYtmxZ5bni4mIjIiLCePHFFy2oEK3l4s+CYRjGzJkzjUmTJllSD6xz6tQpQ5KxZcsWwzD4XghWF38ODIPvhGDWpUsX4+WXX/bL7wNGhlCv/fv3Kzo6Wv369dPUqVN18OBBq0uChQ4dOqT8/HzdcsstlefCwsJ044036qOPPrKwMlglMzNTPXr00BVXXKF77rlHp06dsroktLCCggJJUmRkpCS+F4LVxZ8DD74Tgovb7VZqaqrOnTsnl8vll98HhCHUaeTIkfrzn/+sd999Vy+99JLy8/M1evRonT592urSYJH8/HxJUs+ePaud79mzZ+VjCB7jx4/X66+/rs2bN2v58uXKysrSuHHjVFJSYnVpaCGGYejhhx/WmDFjNHjwYEl8LwSj2j4HEt8JwWTXrl0KDw9XWFiY7r//fq1du1aDBg3yy++DtlYXAPsaP3585fGQIUPkcrnUv39/vfrqq3r44YctrAxWczgc1e4bhlHjHALfHXfcUXk8ePBgDR8+XJdeeqn+8Y9/aPLkyRZWhpbywAMP6F//+pc++OCDGo/xvRA86voc8J0QPAYOHKidO3fqzJkzWrNmjWbOnKktW7ZUPu5P3weMDMFrHTt21JAhQ7R//36rS4FFPLsJXvyvO6dOnarxr0AIPlFRUbr00kv5jghQDz74oNatW6eMjAzFxMRUnud7IbjU9TmoDd8JgSs0NFQDBgzQ8OHDlZycrKFDh2rlypV++X1AGILXSkpKtGfPHkVFRVldCizSr18/9erVSxs3bqw8V1paqi1btmj06NEWVgY7OH36tI4ePcp3RIAxDEMPPPCA0tLStHnzZvXr16/a43wvBIeGPge14TsheBiGoZKSEr/8PmCaHOr06KOPasKECerTp49OnTqlJ598UoWFhZo5c6bVpaEFnT17VgcOHKi8f+jQIe3cuVORkZHq06eP5s2bp6VLl+ryyy/X5ZdfrqVLl+qSSy7RtGnTLKwaLaG+z0JkZKSeeOIJ/fjHP1ZUVJQOHz6sRYsWqVu3bvrRj35kYdVobnPmzNFf//pX/f3vf1enTp0q/8U3IiJCHTp0kMPh4HshCDT0OTh79izfCUFi0aJFGj9+vGJjY1VUVKTU1FRlZmYqPT3dP78PrNvIDnZ3xx13GFFRUUa7du2M6OhoY/Lkycbu3butLgstLCMjw5BU42fmzJmGYZjb6D7++ONGr169jLCwMOOGG24wdu3aZW3RaBH1fRbOnz9v3HLLLUb37t2Ndu3aGX369DFmzpxp5ObmWl02mlltnwFJxp/+9KfKa/heCHwNfQ74Tgge//Vf/2VceumlRmhoqNG9e3fj5ptvNt57773Kx/3t+8BhGIbRmuELAAAAAOyANUMAAAAAghJhCAAAAEBQIgwBAAAACEqEIQAAAABBiTAEAAAAICgRhgAAAAAEJcIQAAAAgKBEGAIAAAAQlAhDAAAAAIISYQgAAABAUCIMAQAAAAhK/x8yziv8oXp+wgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot RAG-Token (falls Ergebnisse da sind)\n",
    "if results[\"rag-token\"][\"scores\"]:\n",
    "    plt.plot(\n",
    "        results[\"rag-token\"][\"k\"], \n",
    "        results[\"rag-token\"][\"scores\"], \n",
    "        marker='o', \n",
    "        label='Fine-Tuned RAG-Token', \n",
    "        linestyle='-', \n",
    "        color='blue'\n",
    "    )\n",
    "\n",
    "# Plot RAG-Sequence (falls Ergebnisse da sind)\n",
    "if results[\"rag-sequence\"][\"scores\"]:\n",
    "    plt.plot(\n",
    "        results[\"rag-sequence\"][\"k\"], \n",
    "        results[\"rag-sequence\"][\"scores\"], \n",
    "        marker='s', \n",
    "        label='Fine-Tuned RAG-Sequence', \n",
    "        linestyle='--', \n",
    "        color='green'\n",
    "    )\n",
    "\n",
    "plt.title(f'RAG Performance on WebQuestions (Fine-Tuned) vs Retrieved Docs (k)\\n(Samples: {len(questions)})')\n",
    "plt.xlabel('Number of Retrieved Documents (k)')\n",
    "plt.ylabel('Exact Match (EM) Score %')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xticks(K_VALUES)\n",
    "\n",
    "plt.savefig(\"wq_k_performance_finetuned.png\")\n",
    "print(\"Plot saved as 'wq_k_performance_finetuned.png'\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
