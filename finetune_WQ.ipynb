{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80cfe237-ef36-4e15-b539-9d50700832db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/micromamba/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/micromamba/lib/python3.11/site-packages (1.13.2)\n",
      "Requirement already satisfied: tqdm in /opt/micromamba/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/micromamba/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/micromamba/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/micromamba/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/micromamba/lib/python3.11/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/micromamba/lib/python3.11/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/micromamba/lib/python3.11/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/micromamba/lib/python3.11/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/micromamba/lib/python3.11/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/micromamba/lib/python3.11/site-packages (from triton==3.4.0->torch) (80.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/micromamba/lib/python3.11/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/micromamba/lib/python3.11/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/micromamba/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pyarrow==14.0.1 in /opt/micromamba/lib/python3.11/site-packages (14.0.1)\n",
      "Requirement already satisfied: datasets==2.14.6 in /opt/micromamba/lib/python3.11/site-packages (2.14.6)\n",
      "Requirement already satisfied: transformers==4.35.2 in /opt/micromamba/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: accelerate==0.24.1 in /opt/micromamba/lib/python3.11/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/micromamba/lib/python3.11/site-packages (from pyarrow==14.0.1) (1.26.4)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/micromamba/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.14.6) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (3.12.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (0.35.0)\n",
      "Requirement already satisfied: packaging in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (6.0.2)\n",
      "Requirement already satisfied: filelock in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (3.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (0.6.2)\n",
      "Requirement already satisfied: psutil in /opt/micromamba/lib/python3.11/site-packages (from accelerate==0.24.1) (6.1.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/micromamba/lib/python3.11/site-packages (from accelerate==0.24.1) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/micromamba/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/micromamba/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (1.1.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.14.6) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.6) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.6) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.6) (2025.8.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/micromamba/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.10.0->accelerate==0.24.1) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/micromamba/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.24.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate==0.24.1) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/micromamba/lib/python3.11/site-packages (from pandas->datasets==2.14.6) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/micromamba/lib/python3.11/site-packages (from pandas->datasets==2.14.6) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/micromamba/lib/python3.11/site-packages (from pandas->datasets==2.14.6) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/micromamba/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch faiss-cpu tqdm\n",
    "\n",
    "!pip install pyarrow==14.0.1 datasets==2.14.6 transformers==4.35.2 accelerate==0.24.1\n",
    "\n",
    "!pip -q install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a048da-fb0b-4fe8-b64d-8a5e3e5e4313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2026-01-26 22:15:38.742411: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Saving all runs under: WQ_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from tqdm.std import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    RagTokenForGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    default_data_collator\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Configurable inputs\n",
    "DATASET_NAME = \"stanfordnlp/web_questions\"\n",
    "SPLIT_TRAIN = \"train\"\n",
    "SPLIT_TEST = \"test\"\n",
    "\n",
    "USE_DUMMY = False\n",
    "OUT_ROOT = \"WQ_models\"\n",
    "\n",
    "# Training hyperparams\n",
    "TRAIN_N_DOCS = 10\n",
    "MAX_Q_LEN = 64\n",
    "MAX_A_LEN = 32\n",
    "LR = 1e-5\n",
    "EPOCHS = 2\n",
    "BSZ = 1\n",
    "GRAD_ACC = 8\n",
    "SAVE_STEPS = 500\n",
    "LOG_STEPS = 50\n",
    "\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "print(\"Saving all runs under:\", OUT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0be05a-eda0-4512-909a-99e9db814ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening stanfordnlp/web_questions:train: 100%|██████████| 3778/3778 [00:00<00:00, 38368.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat train: Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 8933\n",
      "})\n",
      "Test: Dataset({\n",
      "    features: ['url', 'question', 'answers'],\n",
      "    num_rows: 2032\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def build_wq_flat(split: str, dataset_name: str = DATASET_NAME) -> Dataset:\n",
    "    ds = load_dataset(dataset_name, split=split)\n",
    "\n",
    "    questions, answers = [], []\n",
    "    for ex in tqdm(ds, desc=f\"Flattening {dataset_name}:{split}\"):\n",
    "        q = ex[\"question\"]\n",
    "        for a in ex[\"answers\"]:\n",
    "            questions.append(q)\n",
    "            answers.append(a)\n",
    "\n",
    "    return Dataset.from_dict({\"question\": questions, \"answer\": answers})\n",
    "\n",
    "wq_train_flat = build_wq_flat(split=SPLIT_TRAIN)\n",
    "wq_test = load_dataset(DATASET_NAME, split=SPLIT_TEST)\n",
    "\n",
    "print(\"Flat train:\", wq_train_flat)\n",
    "print(\"Test:\", wq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed4a90e-8960-481e-877a-4e61b42a1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "class RagFixedLossTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # outputs can be dict-like or have .loss\n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs.loss\n",
    "\n",
    "        # Make sure it's a scalar\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e534daa7-d90f-43e5-9437-10bf33a22587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocess_fn(\n",
    "    tokenizer: RagTokenizer,\n",
    "    max_q_len: int,\n",
    "    max_a_len: int,\n",
    "    decoder_start_token_id: int,\n",
    "    rag_type: str   # \"token\" or \"sequence\"\n",
    "):\n",
    "    assert rag_type in {\"token\", \"sequence\"}\n",
    "\n",
    "    gen_tok = tokenizer.generator\n",
    "    pad_id = gen_tok.pad_token_id\n",
    "\n",
    "    def preprocess(batch):\n",
    "        # Question encoder inputs\n",
    "        q_enc = tokenizer(\n",
    "            batch[\"question\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_q_len,\n",
    "        )\n",
    "\n",
    "        # Generator target ids (answer_ids)\n",
    "        a_enc = gen_tok(\n",
    "            batch[\"answer\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_a_len,\n",
    "        )\n",
    "        answer_ids = a_enc[\"input_ids\"]\n",
    "\n",
    "        # Decoder_input_ids (shift-right, keep pad ids)\n",
    "        answer_ids = a_enc[\"input_ids\"]\n",
    "\n",
    "        decoder_input_ids = [[decoder_start_token_id] + seq[:-1] for seq in answer_ids]\n",
    "        decoder_attention_mask = [[0 if t == pad_id else 1 for t in seq] for seq in decoder_input_ids]\n",
    "\n",
    "        labels = answer_ids\n",
    "\n",
    "        q_enc[\"decoder_input_ids\"] = decoder_input_ids\n",
    "        q_enc[\"decoder_attention_mask\"] = decoder_attention_mask\n",
    "        q_enc[\"labels\"] = labels\n",
    "        return q_enc\n",
    "\n",
    "    return preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d787f258-85eb-4429-921e-d3a91ed9df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_wq(\n",
    "    base_model_name: str,\n",
    "    rag_type: str,  # \"token\" or \"sequence\"\n",
    "    train_dataset: Dataset,\n",
    "    use_dummy: bool,\n",
    "    out_root: str,\n",
    "    train_n_docs: int,\n",
    "):\n",
    "    assert rag_type in {\"token\", \"sequence\"}\n",
    "\n",
    "    # Timestamped run dir\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_name = f\"{base_model_name.replace('/','_')}__{rag_type}__wq_ft__nDocs{train_n_docs}__{ts}\"\n",
    "    out_dir = os.path.join(out_root, run_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n=== Fine-tuning {rag_type.upper()} from {base_model_name} ===\")\n",
    "    print(f\"Saving to: {out_dir}\")\n",
    "\n",
    "    print(\"Loading tokenizer.\")\n",
    "    tokenizer = RagTokenizer.from_pretrained(base_model_name)\n",
    "    print(\"Tokenizer loaded.\")\n",
    "\n",
    "    print(\"Loading retriever (downloads/loads Wikipedia index if use_dummy=False).\")\n",
    "    retriever = RagRetriever.from_pretrained(\n",
    "        base_model_name,\n",
    "        index_name=\"exact\",\n",
    "        use_dummy_dataset=use_dummy,\n",
    "    )\n",
    "    print(\"Retriever loaded.\")\n",
    "\n",
    "    print(\"Loading model weights.\")\n",
    "    model_cls = RagTokenForGeneration if rag_type == \"token\" else RagSequenceForGeneration\n",
    "    model = model_cls.from_pretrained(base_model_name, retriever=retriever).to(device)\n",
    "    print(\"Model loaded and moved to device.\")\n",
    "\n",
    "    model.config.n_docs = train_n_docs\n",
    "    model.config.use_cache = False\n",
    "    if rag_type == \"token\":\n",
    "        model.config.reduce_loss = True\n",
    "\n",
    "    # Preprocess dataset\n",
    "    decoder_start_id = model.generator.config.decoder_start_token_id\n",
    "    preprocess_fn = make_preprocess_fn(\n",
    "        tokenizer=tokenizer,\n",
    "        max_q_len=MAX_Q_LEN,\n",
    "        max_a_len=MAX_A_LEN,\n",
    "        decoder_start_token_id=decoder_start_id,\n",
    "        rag_type=rag_type\n",
    "    )\n",
    "    train_tok = train_dataset.map(\n",
    "        preprocess_fn,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names\n",
    "    )\n",
    "\n",
    "    train_tok.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    # Training args\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        per_device_train_batch_size=BSZ,\n",
    "        gradient_accumulation_steps=GRAD_ACC,\n",
    "        learning_rate=LR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=LOG_STEPS,\n",
    "        logging_first_step=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=SAVE_STEPS,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "        evaluation_strategy=\"no\",\n",
    "        predict_with_generate=False,\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = RagFixedLossTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_tok,\n",
    "        data_collator=default_data_collator,\n",
    "        tokenizer=None\n",
    "    )\n",
    "\n",
    "    print(\"Number train examples:\", len(train_tok))\n",
    "    approx_steps = (len(train_tok) // (BSZ * GRAD_ACC)) * EPOCHS\n",
    "    print(\"Expected steps (approx):\", approx_steps)\n",
    "    print(\"Starting training loop now...\")\n",
    "\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    print(train_result)\n",
    "\n",
    "    # Save model + tokenizer to SAME folder\n",
    "    trainer.save_model(out_dir)\n",
    "    tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "    # Save meta\n",
    "    meta = {\n",
    "        \"dataset\": DATASET_NAME,\n",
    "        \"split_train\": SPLIT_TRAIN,\n",
    "        \"split_test\": SPLIT_TEST,\n",
    "        \"base_model_name\": base_model_name,\n",
    "        \"rag_type\": rag_type,\n",
    "        \"use_dummy_dataset\": use_dummy,\n",
    "        \"train_n_docs\": train_n_docs,\n",
    "        \"max_q_len\": MAX_Q_LEN,\n",
    "        \"max_a_len\": MAX_A_LEN,\n",
    "        \"learning_rate\": LR,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BSZ,\n",
    "        \"grad_accumulation\": GRAD_ACC,\n",
    "    }\n",
    "    with open(os.path.join(out_dir, \"run_meta.json\"), \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"Saved fine-tuned checkpoint to {out_dir}\")\n",
    "    return out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "508cbd87-2402-4111-8c15-72fe63789524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fine-tuning TOKEN from facebook/rag-token-nq ===\n",
      "Saving to: WQ_models/facebook_rag-token-nq__token__wq_ft__nDocs10__20260126_221543\n",
      "Loading tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/transformers/models/bart/configuration_bart.py:179: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n",
      "Loading retriever (downloads/loads Wikipedia index if use_dummy=False).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/66fd9b80f51375c02cd9010050e781ed3e8f759e868f690c31b2686a7a0eeb5c (last modified on Thu Jan 15 21:07:41 2026) since it couldn't be found locally at wiki_dpr., or remotely on the Hugging Face Hub.\n",
      "/opt/micromamba/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/66fd9b80f51375c02cd9010050e781ed3e8f759e868f690c31b2686a7a0eeb5c (last modified on Thu Jan 15 21:07:41 2026) since it couldn't be found locally at wiki_dpr., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever loaded.\n",
      "Loading model weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to device.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837f8037fa334242b0dd5a9a2649f4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8933 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/accelerate/accelerator.py:439: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train examples: 8933\n",
      "Expected steps (approx): 2232\n",
      "Starting training loop now...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2232' max='2232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2232/2232 2:06:50, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>38.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>43.511800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>31.818900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>28.742300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>28.229000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>26.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>24.270100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>24.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>24.540800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>23.825800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>25.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>22.365500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>22.102100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>23.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>23.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>21.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>21.948900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>19.721500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>23.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>21.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>19.915100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>20.171700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>20.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>19.609200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>18.879500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>18.677200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>18.565200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>17.506700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>18.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>16.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>18.766100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>17.965900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>17.965600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>17.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>18.155300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>18.990100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>17.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>17.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>19.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>17.311400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>17.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>17.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>17.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>18.608400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>17.727900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "TrainOutput(global_step=2232, training_loss=21.23163855802201, metrics={'train_runtime': 7614.0266, 'train_samples_per_second': 2.346, 'train_steps_per_second': 0.293, 'total_flos': 3001692551380992.0, 'train_loss': 21.23163855802201, 'epoch': 2.0})\n",
      "Saved fine-tuned checkpoint to WQ_models/facebook_rag-token-nq__token__wq_ft__nDocs10__20260126_221543\n",
      "RAG Token checkpoint: WQ_models/facebook_rag-token-nq__token__wq_ft__nDocs10__20260126_221543\n"
     ]
    }
   ],
   "source": [
    "token_ckpt_dir = finetune_wq(\n",
    "    base_model_name=\"facebook/rag-token-nq\",\n",
    "    rag_type=\"token\",\n",
    "    train_dataset=wq_train_flat,\n",
    "    use_dummy=USE_DUMMY,\n",
    "    out_root=OUT_ROOT,\n",
    "    train_n_docs=TRAIN_N_DOCS,\n",
    ")\n",
    "print(\"RAG Token checkpoint:\", token_ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51158b08-73b4-4c32-b9bf-2aa855c92985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fine-tuning SEQUENCE from facebook/rag-sequence-nq ===\n",
      "Saving to: WQ_models/facebook_rag-sequence-nq__sequence__wq_ft__nDocs10__20260127_002859\n",
      "Loading tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n",
      "Loading retriever (downloads/loads Wikipedia index if use_dummy=False).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/66fd9b80f51375c02cd9010050e781ed3e8f759e868f690c31b2686a7a0eeb5c (last modified on Thu Jan 15 21:07:41 2026) since it couldn't be found locally at wiki_dpr., or remotely on the Hugging Face Hub.\n",
      "/opt/micromamba/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/66fd9b80f51375c02cd9010050e781ed3e8f759e868f690c31b2686a7a0eeb5c (last modified on Thu Jan 15 21:07:41 2026) since it couldn't be found locally at wiki_dpr., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever loaded.\n",
      "Loading model weights.\n",
      "Model loaded and moved to device.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2885aee0154ec59953498d6602f43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8933 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/accelerate/accelerator.py:439: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train examples: 8933\n",
      "Expected steps (approx): 2232\n",
      "Starting training loop now...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2232' max='2232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2232/2232 2:11:36, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>46.913400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>30.947200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>16.632800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>15.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>15.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>13.639200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>12.522500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>12.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>12.320800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>12.635800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>13.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>11.844600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>11.063600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>11.939100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>11.855300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>10.573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>11.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>9.674200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>11.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>10.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>10.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>10.086500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>10.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>9.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>8.486500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>8.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>8.828100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>8.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>8.087100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>7.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>8.644500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>8.330900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>8.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>8.203400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>8.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>8.966600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>7.884600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>8.205800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>8.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>7.916700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>8.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>7.909100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>8.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>8.604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>7.534300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "TrainOutput(global_step=2232, training_loss=10.604088774718692, metrics={'train_runtime': 7899.9946, 'train_samples_per_second': 2.262, 'train_steps_per_second': 0.283, 'total_flos': 3001692551380992.0, 'train_loss': 10.604088774718692, 'epoch': 2.0})\n",
      "Saved fine-tuned checkpoint to WQ_models/facebook_rag-sequence-nq__sequence__wq_ft__nDocs10__20260127_002859\n",
      "RAG Sequence checkpoint: WQ_models/facebook_rag-sequence-nq__sequence__wq_ft__nDocs10__20260127_002859\n"
     ]
    }
   ],
   "source": [
    "seq_ckpt_dir = finetune_wq(\n",
    "    base_model_name=\"facebook/rag-sequence-nq\",\n",
    "    rag_type=\"sequence\",\n",
    "    train_dataset=wq_train_flat,\n",
    "    use_dummy=USE_DUMMY,\n",
    "    out_root=OUT_ROOT,\n",
    "    train_n_docs=TRAIN_N_DOCS,\n",
    ")\n",
    "print(\"RAG Sequence checkpoint:\", seq_ckpt_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
