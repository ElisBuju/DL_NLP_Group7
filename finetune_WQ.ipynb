{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80cfe237-ef36-4e15-b539-9d50700832db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/micromamba/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/micromamba/lib/python3.11/site-packages (1.13.2)\n",
      "Requirement already satisfied: tqdm in /opt/micromamba/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/micromamba/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/micromamba/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/micromamba/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/micromamba/lib/python3.11/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/micromamba/lib/python3.11/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/micromamba/lib/python3.11/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/micromamba/lib/python3.11/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/micromamba/lib/python3.11/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/micromamba/lib/python3.11/site-packages (from triton==3.4.0->torch) (80.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/micromamba/lib/python3.11/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/micromamba/lib/python3.11/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/micromamba/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pyarrow==14.0.1 in /opt/micromamba/lib/python3.11/site-packages (14.0.1)\n",
      "Requirement already satisfied: datasets==2.14.6 in /opt/micromamba/lib/python3.11/site-packages (2.14.6)\n",
      "Requirement already satisfied: transformers==4.35.2 in /opt/micromamba/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: accelerate==0.24.1 in /opt/micromamba/lib/python3.11/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/micromamba/lib/python3.11/site-packages (from pyarrow==14.0.1) (1.26.4)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/micromamba/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.14.6) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (3.12.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (0.35.0)\n",
      "Requirement already satisfied: packaging in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (6.0.2)\n",
      "Requirement already satisfied: filelock in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (3.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (0.6.2)\n",
      "Requirement already satisfied: psutil in /opt/micromamba/lib/python3.11/site-packages (from accelerate==0.24.1) (6.1.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/micromamba/lib/python3.11/site-packages (from accelerate==0.24.1) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/micromamba/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/micromamba/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (1.1.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.14.6) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.6) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.6) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.6) (2025.8.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/micromamba/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.10.0->accelerate==0.24.1) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/micromamba/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.24.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate==0.24.1) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/micromamba/lib/python3.11/site-packages (from pandas->datasets==2.14.6) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/micromamba/lib/python3.11/site-packages (from pandas->datasets==2.14.6) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/micromamba/lib/python3.11/site-packages (from pandas->datasets==2.14.6) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/micromamba/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch faiss-cpu tqdm\n",
    "\n",
    "!pip install pyarrow==14.0.1 datasets==2.14.6 transformers==4.35.2 accelerate==0.24.1\n",
    "\n",
    "!pip -q install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a048da-fb0b-4fe8-b64d-8a5e3e5e4313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2026-01-17 19:11:41.731091: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-17 19:11:41.789610: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Saving all runs under: WQ_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from tqdm.std import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    RagTokenForGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    default_data_collator\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Configurable inputs\n",
    "DATASET_NAME = \"stanfordnlp/web_questions\"\n",
    "SPLIT_TRAIN = \"train\"\n",
    "SPLIT_TEST = \"test\"\n",
    "\n",
    "USE_DUMMY = True\n",
    "OUT_ROOT = \"WQ_models\"\n",
    "\n",
    "# Training hyperparams\n",
    "TRAIN_N_DOCS = 1\n",
    "MAX_Q_LEN = 64\n",
    "MAX_A_LEN = 32\n",
    "LR = 1e-5\n",
    "EPOCHS = 2\n",
    "BSZ = 1\n",
    "GRAD_ACC = 8\n",
    "SAVE_STEPS = 500\n",
    "LOG_STEPS = 50\n",
    "\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "print(\"Saving all runs under:\", OUT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0be05a-eda0-4512-909a-99e9db814ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening stanfordnlp/web_questions:train: 100%|██████████| 3778/3778 [00:00<00:00, 29447.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat train: Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 8933\n",
      "})\n",
      "Test: Dataset({\n",
      "    features: ['url', 'question', 'answers'],\n",
      "    num_rows: 2032\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def build_wq_flat(split: str, dataset_name: str = DATASET_NAME) -> Dataset:\n",
    "    ds = load_dataset(dataset_name, split=split)\n",
    "\n",
    "    questions, answers = [], []\n",
    "    for ex in tqdm(ds, desc=f\"Flattening {dataset_name}:{split}\"):\n",
    "        q = ex[\"question\"]\n",
    "        for a in ex[\"answers\"]:\n",
    "            questions.append(q)\n",
    "            answers.append(a)\n",
    "\n",
    "    return Dataset.from_dict({\"question\": questions, \"answer\": answers})\n",
    "\n",
    "\n",
    "wq_train_flat = build_wq_flat(split=SPLIT_TRAIN)\n",
    "wq_test = load_dataset(DATASET_NAME, split=SPLIT_TEST)\n",
    "\n",
    "print(\"Flat train:\", wq_train_flat)\n",
    "print(\"Test:\", wq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed4a90e-8960-481e-877a-4e61b42a1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "class RagFixedLossTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # outputs can be dict-like or have .loss\n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs.loss\n",
    "\n",
    "        # Make sure it's a scalar\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e534daa7-d90f-43e5-9437-10bf33a22587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocess_fn(\n",
    "    tokenizer: RagTokenizer,\n",
    "    max_q_len: int,\n",
    "    max_a_len: int,\n",
    "    decoder_start_token_id: int,\n",
    "    rag_type: str   # \"token\" or \"sequence\"\n",
    "):\n",
    "    assert rag_type in {\"token\", \"sequence\"}\n",
    "\n",
    "    gen_tok = tokenizer.generator\n",
    "    pad_id = gen_tok.pad_token_id\n",
    "\n",
    "    def preprocess(batch):\n",
    "        # Question encoder inputs\n",
    "        q_enc = tokenizer(\n",
    "            batch[\"question\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_q_len,\n",
    "        )\n",
    "\n",
    "        # Generator target ids (answer_ids)\n",
    "        a_enc = gen_tok(\n",
    "            batch[\"answer\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_a_len,\n",
    "        )\n",
    "        answer_ids = a_enc[\"input_ids\"]\n",
    "\n",
    "        # Decoder_input_ids (shift-right, keep pad ids)\n",
    "        answer_ids = a_enc[\"input_ids\"]\n",
    "\n",
    "        decoder_input_ids = [[decoder_start_token_id] + seq[:-1] for seq in answer_ids]\n",
    "        decoder_attention_mask = [[0 if t == pad_id else 1 for t in seq] for seq in decoder_input_ids]\n",
    "\n",
    "        labels = answer_ids\n",
    "\n",
    "        q_enc[\"decoder_input_ids\"] = decoder_input_ids\n",
    "        q_enc[\"decoder_attention_mask\"] = decoder_attention_mask\n",
    "        q_enc[\"labels\"] = labels\n",
    "        return q_enc\n",
    "\n",
    "    return preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d787f258-85eb-4429-921e-d3a91ed9df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_wq(\n",
    "    base_model_name: str,\n",
    "    rag_type: str,  # \"token\" or \"sequence\"\n",
    "    train_dataset: Dataset,\n",
    "    use_dummy: bool,\n",
    "    out_root: str,\n",
    "    train_n_docs: int,\n",
    "):\n",
    "    assert rag_type in {\"token\", \"sequence\"}\n",
    "\n",
    "    # Timestamped run dir\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_name = f\"{base_model_name.replace('/','_')}__{rag_type}__wq_ft__nDocs{train_n_docs}__{ts}\"\n",
    "    out_dir = os.path.join(out_root, run_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n=== Fine-tuning {rag_type.upper()} from {base_model_name} ===\")\n",
    "    print(f\"Saving to: {out_dir}\")\n",
    "\n",
    "    print(\"Loading tokenizer.\")\n",
    "    tokenizer = RagTokenizer.from_pretrained(base_model_name)\n",
    "    print(\"Tokenizer loaded.\")\n",
    "\n",
    "    print(\"Loading retriever (downloads/loads Wikipedia index if use_dummy=False).\")\n",
    "    retriever = RagRetriever.from_pretrained(\n",
    "        base_model_name,\n",
    "        index_name=\"exact\",\n",
    "        use_dummy_dataset=use_dummy,\n",
    "    )\n",
    "    print(\"Retriever loaded.\")\n",
    "\n",
    "    print(\"Loading model weights.\")\n",
    "    model_cls = RagTokenForGeneration if rag_type == \"token\" else RagSequenceForGeneration\n",
    "    model = model_cls.from_pretrained(base_model_name, retriever=retriever).to(device)\n",
    "    print(\"Model loaded and moved to device.\")\n",
    "\n",
    "    model.config.n_docs = train_n_docs\n",
    "    model.config.use_cache = False\n",
    "    if rag_type == \"token\":\n",
    "        model.config.reduce_loss = True\n",
    "\n",
    "    # Preprocess dataset\n",
    "    decoder_start_id = model.generator.config.decoder_start_token_id\n",
    "    preprocess_fn = make_preprocess_fn(\n",
    "        tokenizer=tokenizer,\n",
    "        max_q_len=MAX_Q_LEN,\n",
    "        max_a_len=MAX_A_LEN,\n",
    "        decoder_start_token_id=decoder_start_id,\n",
    "        rag_type=rag_type\n",
    "    )\n",
    "    train_tok = train_dataset.map(\n",
    "        preprocess_fn,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names\n",
    "    )\n",
    "\n",
    "    train_tok.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    # Training args\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        per_device_train_batch_size=BSZ,\n",
    "        gradient_accumulation_steps=GRAD_ACC,\n",
    "        learning_rate=LR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=LOG_STEPS,\n",
    "        logging_first_step=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=SAVE_STEPS,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "        evaluation_strategy=\"no\",\n",
    "        predict_with_generate=False,\n",
    "        remove_unused_columns=False,\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = RagFixedLossTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_tok,\n",
    "        data_collator=default_data_collator,\n",
    "        tokenizer=None\n",
    "    )\n",
    "\n",
    "    print(\"Number train examples:\", len(train_tok))\n",
    "    approx_steps = (len(train_tok) // (BSZ * GRAD_ACC)) * EPOCHS\n",
    "    print(\"Expected steps (approx):\", approx_steps)\n",
    "    print(\"Starting training loop now...\")\n",
    "\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    print(train_result)\n",
    "\n",
    "    # Save model + tokenizer to SAME folder\n",
    "    trainer.save_model(out_dir)\n",
    "    tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "    # Save meta\n",
    "    meta = {\n",
    "        \"dataset\": DATASET_NAME,\n",
    "        \"split_train\": SPLIT_TRAIN,\n",
    "        \"split_test\": SPLIT_TEST,\n",
    "        \"base_model_name\": base_model_name,\n",
    "        \"rag_type\": rag_type,\n",
    "        \"use_dummy_dataset\": use_dummy,\n",
    "        \"train_n_docs\": train_n_docs,\n",
    "        \"max_q_len\": MAX_Q_LEN,\n",
    "        \"max_a_len\": MAX_A_LEN,\n",
    "        \"learning_rate\": LR,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BSZ,\n",
    "        \"grad_accumulation\": GRAD_ACC,\n",
    "    }\n",
    "    with open(os.path.join(out_dir, \"run_meta.json\"), \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"Saved fine-tuned checkpoint to {out_dir}\")\n",
    "    return out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "508cbd87-2402-4111-8c15-72fe63789524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fine-tuning TOKEN from facebook/rag-token-nq ===\n",
      "Saving to: WQ_models/facebook_rag-token-nq__token__wq_ft__nDocs1__20260117_191147\n",
      "Loading tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/transformers/models/bart/configuration_bart.py:179: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n",
      "Loading retriever (downloads/loads Wikipedia index if use_dummy=False).\n",
      "Retriever loaded.\n",
      "Loading model weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to device.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc4858287ae411eba02e9941acb5454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8933 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/accelerate/accelerator.py:439: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train examples: 8933\n",
      "Expected steps (approx): 2232\n",
      "Starting training loop now...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='2232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  50/2232 00:59 < 45:12, 0.80 it/s, Epoch 0.04/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>41.191700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m token_ckpt_dir = \u001b[43mfinetune_wq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebook/rag-token-nq\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrag_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtoken\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwq_train_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_dummy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_DUMMY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUT_ROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_n_docs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAIN_N_DOCS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRAG Token checkpoint:\u001b[39m\u001b[33m\"\u001b[39m, token_ckpt_dir)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mfinetune_wq\u001b[39m\u001b[34m(base_model_name, rag_type, train_dataset, use_dummy, out_root, train_n_docs)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExpected steps (approx):\u001b[39m\u001b[33m\"\u001b[39m, approx_steps)\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training loop now...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m \u001b[38;5;28mprint\u001b[39m(train_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/trainer.py:1555\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1553\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1554\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1555\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1556\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/trainer.py:1860\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   1857\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   1859\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m1860\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1863\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   1864\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[32m   1865\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   1866\u001b[39m ):\n\u001b[32m   1867\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   1868\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/trainer.py:2734\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs)\u001b[39m\n\u001b[32m   2732\u001b[39m         scaled_loss.backward()\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2736\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach() / \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/accelerate/accelerator.py:1987\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1986\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1987\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1989\u001b[39m     loss.backward(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "token_ckpt_dir = finetune_wq(\n",
    "    base_model_name=\"facebook/rag-token-nq\",\n",
    "    rag_type=\"token\",\n",
    "    train_dataset=wq_train_flat,\n",
    "    use_dummy=USE_DUMMY,\n",
    "    out_root=OUT_ROOT,\n",
    "    train_n_docs=TRAIN_N_DOCS,\n",
    ")\n",
    "print(\"RAG Token checkpoint:\", token_ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51158b08-73b4-4c32-b9bf-2aa855c92985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fine-tuning SEQUENCE from facebook/rag-sequence-nq ===\n",
      "Saving to: WQ_models/facebook_rag-sequence-nq__sequence__wq_ft__nDocs1__20260117_191308\n",
      "Loading tokenizer.\n",
      "Tokenizer loaded.\n",
      "Loading retriever (downloads/loads Wikipedia index if use_dummy=False).\n",
      "Retriever loaded.\n",
      "Loading model weights.\n",
      "Model loaded and moved to device.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b4f9788ed143d9bedaae50d59bc42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8933 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train examples: 8933\n",
      "Expected steps (approx): 2232\n",
      "Starting training loop now...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='2232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   7/2232 00:05 < 41:31, 0.89 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>49.496400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 14.57 GiB of which 4.75 MiB is free. Process 1135247 has 14.56 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 360.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m seq_ckpt_dir = \u001b[43mfinetune_wq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebook/rag-sequence-nq\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrag_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msequence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwq_train_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_dummy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_DUMMY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUT_ROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_n_docs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAIN_N_DOCS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRAG Sequence checkpoint:\u001b[39m\u001b[33m\"\u001b[39m, seq_ckpt_dir)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mfinetune_wq\u001b[39m\u001b[34m(base_model_name, rag_type, train_dataset, use_dummy, out_root, train_n_docs)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExpected steps (approx):\u001b[39m\u001b[33m\"\u001b[39m, approx_steps)\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training loop now...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m \u001b[38;5;28mprint\u001b[39m(train_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/trainer.py:1555\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1553\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1554\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1555\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1556\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/transformers/trainer.py:1910\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   1904\u001b[39m         \u001b[38;5;28mself\u001b[39m.accelerator.clip_grad_norm_(\n\u001b[32m   1905\u001b[39m             model.parameters(),\n\u001b[32m   1906\u001b[39m             args.max_grad_norm,\n\u001b[32m   1907\u001b[39m         )\n\u001b[32m   1909\u001b[39m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1910\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1911\u001b[39m optimizer_was_run = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.optimizer_step_was_skipped\n\u001b[32m   1912\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run:\n\u001b[32m   1913\u001b[39m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/accelerate/optimizer.py:132\u001b[39m, in \u001b[36mAcceleratedOptimizer.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.step = \u001b[38;5;28mself\u001b[39m._optimizer_patched_step_method\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mself\u001b[39m.scaler.update()\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accelerate_step_called:\n\u001b[32m    136\u001b[39m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/amp/grad_scaler.py:465\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    459\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    462\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    463\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/amp/grad_scaler.py:360\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v.item() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     retval = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/accelerate/optimizer.py:185\u001b[39m, in \u001b[36mpatch_optimizer_step.<locals>.patched_step\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpatched_step\u001b[39m(*args, **kwargs):\n\u001b[32m    184\u001b[39m     accelerated_optimizer._accelerate_step_called = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:133\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m opt = opt_ref()\n\u001b[32m    132\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/optim/optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/optim/optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/optim/adam.py:237\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     state_steps: \u001b[38;5;28mlist\u001b[39m[Tensor] = []\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     has_complex = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m     adam(\n\u001b[32m    248\u001b[39m         params_with_grad,\n\u001b[32m    249\u001b[39m         grads,\n\u001b[32m   (...)\u001b[39m\u001b[32m    268\u001b[39m         decoupled_weight_decay=group[\u001b[33m\"\u001b[39m\u001b[33mdecoupled_weight_decay\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    269\u001b[39m     )\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/micromamba/lib/python3.11/site-packages/torch/optim/adam.py:177\u001b[39m, in \u001b[36mAdam._init_group\u001b[39m\u001b[34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[39m\n\u001b[32m    167\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m] = (\n\u001b[32m    168\u001b[39m     torch.zeros(\n\u001b[32m    169\u001b[39m         (),\n\u001b[32m   (...)\u001b[39m\u001b[32m    174\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m torch.tensor(\u001b[32m0.0\u001b[39m, dtype=_get_scalar_dtype())\n\u001b[32m    175\u001b[39m )\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mexp_avg\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreserve_format\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[32m    181\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mexp_avg_sq\u001b[39m\u001b[33m\"\u001b[39m] = torch.zeros_like(\n\u001b[32m    182\u001b[39m     p, memory_format=torch.preserve_format\n\u001b[32m    183\u001b[39m )\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 14.57 GiB of which 4.75 MiB is free. Process 1135247 has 14.56 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 360.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "seq_ckpt_dir = finetune_wq(\n",
    "    base_model_name=\"facebook/rag-sequence-nq\",\n",
    "    rag_type=\"sequence\",\n",
    "    train_dataset=wq_train_flat,\n",
    "    use_dummy=USE_DUMMY,\n",
    "    out_root=OUT_ROOT,\n",
    "    train_n_docs=TRAIN_N_DOCS,\n",
    ")\n",
    "print(\"RAG Sequence checkpoint:\", seq_ckpt_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
