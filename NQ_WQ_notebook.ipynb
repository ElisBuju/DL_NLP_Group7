{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80cfe237-ef36-4e15-b539-9d50700832db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/micromamba/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/micromamba/lib/python3.11/site-packages (1.13.2)\n",
      "Requirement already satisfied: tqdm in /opt/micromamba/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/micromamba/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/micromamba/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/micromamba/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/micromamba/lib/python3.11/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/micromamba/lib/python3.11/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/micromamba/lib/python3.11/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/micromamba/lib/python3.11/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/micromamba/lib/python3.11/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/micromamba/lib/python3.11/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/micromamba/lib/python3.11/site-packages (from triton==3.4.0->torch) (80.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/micromamba/lib/python3.11/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/micromamba/lib/python3.11/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/micromamba/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pyarrow==14.0.1 in /opt/micromamba/lib/python3.11/site-packages (14.0.1)\n",
      "Requirement already satisfied: datasets==2.14.6 in /opt/micromamba/lib/python3.11/site-packages (2.14.6)\n",
      "Requirement already satisfied: transformers==4.35.2 in /opt/micromamba/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: accelerate==0.24.1 in /opt/micromamba/lib/python3.11/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/micromamba/lib/python3.11/site-packages (from pyarrow==14.0.1) (1.26.4)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/micromamba/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.14.6) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (3.12.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (0.35.0)\n",
      "Requirement already satisfied: packaging in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/micromamba/lib/python3.11/site-packages (from datasets==2.14.6) (6.0.2)\n",
      "Requirement already satisfied: filelock in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (3.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/micromamba/lib/python3.11/site-packages (from transformers==4.35.2) (0.6.2)\n",
      "Requirement already satisfied: psutil in /opt/micromamba/lib/python3.11/site-packages (from accelerate==0.24.1) (6.1.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/micromamba/lib/python3.11/site-packages (from accelerate==0.24.1) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/micromamba/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/micromamba/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (1.1.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/micromamba/lib/python3.11/site-packages (from aiohttp->datasets==2.14.6) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.14.6) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.6) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.6) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/micromamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.6) (2025.8.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/micromamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.24.1) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/micromamba/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.10.0->accelerate==0.24.1) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/micromamba/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.24.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/micromamba/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate==0.24.1) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/micromamba/lib/python3.11/site-packages (from pandas->datasets==2.14.6) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/micromamba/lib/python3.11/site-packages (from pandas->datasets==2.14.6) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/micromamba/lib/python3.11/site-packages (from pandas->datasets==2.14.6) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/micromamba/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch faiss-cpu tqdm\n",
    "\n",
    "!pip install pyarrow==14.0.1 datasets==2.14.6 transformers==4.35.2 accelerate==0.24.1\n",
    "\n",
    "!pip -q install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a048da-fb0b-4fe8-b64d-8a5e3e5e4313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "from transformers import (\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    RagTokenForGeneration\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0be05a-eda0-4512-909a-99e9db814ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    \"\"\"Check if the prediction exactly matches the ground truth.\"\"\"\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def calculate_em(predictions, references):\n",
    "    \"\"\"\n",
    "    predictions: list of strings\n",
    "    references: list of lists of strings (since one question can have multiple valid answers)\n",
    "    \"\"\"\n",
    "    total_em = 0\n",
    "    for pred, refs in zip(predictions, references):\n",
    "        # If the prediction matches ANY of the valid references, it's a hit\n",
    "        if any(exact_match_score(pred, gt) for gt in refs):\n",
    "            total_em += 1\n",
    "\n",
    "    return 100 * (total_em / len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508cbd87-2402-4111-8c15-72fe63789524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGModelManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        rag_type,  # \"sequence\" or \"token\"\n",
    "        n_docs,\n",
    "        use_dummy=False,\n",
    "        index_name=\"exact\",\n",
    "        num_beams=1,\n",
    "        max_new_tokens=16,\n",
    "    ):\n",
    "        assert rag_type in [\"sequence\", \"token\"]\n",
    "        self.model_name = model_name\n",
    "        self.rag_type = rag_type\n",
    "\n",
    "        self.n_docs = n_docs\n",
    "        self.num_beams = num_beams\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "        print(f\"Loading Model: {model_name} ({rag_type})...\")\n",
    "\n",
    "        # Load Tokenizer\n",
    "        self.tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Load Retriever\n",
    "        self.retriever = RagRetriever.from_pretrained(\n",
    "            model_name,\n",
    "            index_name=index_name,\n",
    "            use_dummy_dataset=use_dummy\n",
    "        )\n",
    "\n",
    "        # Load RAG Model\n",
    "        if rag_type == \"token\":\n",
    "            self.model = RagTokenForGeneration.from_pretrained(\n",
    "                model_name,\n",
    "                retriever=self.retriever\n",
    "            ).to(device)\n",
    "        else:\n",
    "            self.model = RagSequenceForGeneration.from_pretrained(\n",
    "                model_name,\n",
    "                retriever=self.retriever\n",
    "            ).to(device)\n",
    "\n",
    "        self.model.config.n_docs = self.n_docs\n",
    "\n",
    "        self.model.eval()\n",
    "        print(\"Model loaded successfully.\")\n",
    "        print(f\"Configured n_docs={self.model.config.n_docs}, num_beams={self.num_beams}, max_new_tokens={self.max_new_tokens}\")\n",
    "\n",
    "    def set_n_docs(self, n_docs: int):\n",
    "        \"\"\"\n",
    "        Controls K retrieved docs used by RAG at generation time.\n",
    "        \"\"\"\n",
    "        self.n_docs = n_docs\n",
    "        self.model.config.n_docs = n_docs\n",
    "        if hasattr(self.retriever, \"n_docs\"):\n",
    "            self.retriever.n_docs = n_docs\n",
    "        if hasattr(self.retriever, \"config\"):\n",
    "            self.retriever.config.n_docs = n_docs\n",
    "\n",
    "    def generate_answers(self, questions, batch_size=4):\n",
    "        self.model.eval()\n",
    "        all_answers = []\n",
    "\n",
    "        for i in tqdm(range(0, len(questions), batch_size), desc=\"Generating\"):\n",
    "            batch_questions = questions[i: i + batch_size]\n",
    "\n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                batch_questions,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "\n",
    "            # Generate\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    n_docs=self.n_docs,\n",
    "                    num_beams=self.num_beams,\n",
    "                    do_sample=False,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    min_length=1,\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "\n",
    "            # Decode\n",
    "            batch_answers = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            all_answers.extend(batch_answers)\n",
    "\n",
    "        return all_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47bde038-2de1-4005-8367-0bf7f8c48579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(task_name, model_manager, num_samples=None):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"STARTING EXPERIMENT: {task_name}\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # DATA LOADING\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    if task_name == \"Natural Questions\":\n",
    "        # Load NQ Open (Simplified version)\n",
    "        dataset = load_dataset(\"nq_open\", split=\"validation\")\n",
    "\n",
    "        print(\"Processing Natural Questions dataset...\")\n",
    "        for row in tqdm(dataset, desc=\"Loading Data\"):\n",
    "            q = row['question']\n",
    "            ans_list = row['answer']\n",
    "            if ans_list:\n",
    "                questions.append(q)\n",
    "                answers.append(ans_list)\n",
    "\n",
    "            if num_samples and len(questions) >= num_samples:\n",
    "                break\n",
    "\n",
    "    elif task_name == \"WebQuestions\":\n",
    "        # Load WebQuestions\n",
    "        dataset = load_dataset(\"stanfordnlp/web_questions\", split=\"test\")\n",
    "\n",
    "        print(\"Processing WebQuestions dataset...\")\n",
    "        for row in tqdm(dataset, desc=\"Loading Data\"):\n",
    "            questions.append(row['question'])\n",
    "            answers.append(row['answers'])\n",
    "\n",
    "            if num_samples and len(questions) >= num_samples: \n",
    "                break\n",
    "\n",
    "    print(f\"Loaded {len(questions)} TOTAL samples for {task_name}.\")\n",
    "\n",
    "    # EXECUTION\n",
    "    predictions = model_manager.generate_answers(questions, batch_size=8)\n",
    "\n",
    "    # EVALUATION\n",
    "    score = calculate_em(predictions, answers)\n",
    "\n",
    "    print(f\"\\nRESULTS for {task_name}:\")\n",
    "    print(f\"Exact Match (EM): {score:.2f}%\")\n",
    "\n",
    "    # Save results to a file\n",
    "    with open(f\"{task_name.replace(' ', '_')}_results.txt\", \"w\") as f:\n",
    "        f.write(f\"Task: {task_name}\\n\")\n",
    "        f.write(f\"Samples: {len(questions)}\\n\")\n",
    "        f.write(f\"EM Score: {score:.2f}%\\n\")\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33211add-7920-4376-9fd3-c94887d9df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "# TRUE = Runs fast (prototype), Score will be ~0%\n",
    "# FALSE = Downloads 75GB index\n",
    "USE_DUMMY = False\n",
    "\n",
    "# Set to None to run EVERYTHING\n",
    "# Set to integer (e.g., 50) for testing\n",
    "NUM_SAMPLES = None\n",
    "\n",
    "INDEX_NAME = \"exact\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af6544c-206a-48f5-b81d-a64c10382845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model: facebook/rag-sequence-nq (sequence)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "/opt/micromamba/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Configured n_docs=15, num_beams=1, max_new_tokens=16\n",
      "Experiment: Natural Questions (RAG-Sequence baseline reproduction)\n",
      "\n",
      "========================================\n",
      "STARTING EXPERIMENT: Natural Questions\n",
      "========================================\n",
      "Processing Natural Questions dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████| 3610/3610 [00:00<00:00, 37666.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3610 TOTAL samples for Natural Questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/452 [00:00<?, ?it/s]/opt/micromamba/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Generating: 100%|██████████| 452/452 [4:10:30<00:00, 33.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS for Natural Questions:\n",
      "Exact Match (EM): 38.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Task: Natural Questions (NQ Open) — evaluate Exact Match (EM)\n",
    "# Model: RAG-Sequence NQ checkpoint (facebook/rag-sequence-nq)\n",
    "# Paper mapping: Table 1 \"RAG-Seq.\" on NQ; key knob is K retrieved docs (n_docs)\n",
    "# Goal: Reproduce a comparable EM score trend using HuggingFace’s released checkpoint\n",
    "# ============================================================\n",
    "rag_seq_nq = RAGModelManager(\n",
    "    model_name=\"facebook/rag-sequence-nq\",\n",
    "    rag_type=\"sequence\",\n",
    "    n_docs=15,  # Paper-style setting: RAG-Sequence often benefits from larger K at test time\n",
    "    use_dummy=USE_DUMMY,\n",
    "    index_name=INDEX_NAME,  # \"exact\" uses the built-in Wikipedia DPR index format\n",
    "    num_beams=1,  # Greedy decoding (paper notes greedy was sufficient for QA)\n",
    "    max_new_tokens=16  # QA answers are short; keep generation bounded for speed/consistency\n",
    ")\n",
    "\n",
    "print(\"Experiment: Natural Questions (RAG-Sequence baseline reproduction)\")\n",
    "nq_score_seq = run_experiment(\"Natural Questions\", rag_seq_nq, num_samples=NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb3310b-8ee8-4905-967b-7f97c889379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model: facebook/rag-token-nq (token)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/transformers/models/bart/configuration_bart.py:179: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Configured n_docs=15, num_beams=1, max_new_tokens=16\n",
      "Experiment: Natural Questions (RAG-Token baseline reproduction)\n",
      "\n",
      "========================================\n",
      "STARTING EXPERIMENT: Natural Questions\n",
      "========================================\n",
      "Processing Natural Questions dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████| 3610/3610 [00:00<00:00, 47529.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3610 TOTAL samples for Natural Questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/452 [00:00<?, ?it/s]/opt/micromamba/lib/python3.11/site-packages/transformers/generation/utils.py:2465: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.\n",
      "  warnings.warn(\n",
      "Generating: 100%|██████████| 452/452 [29:51<00:00,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS for Natural Questions:\n",
      "Exact Match (EM): 39.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Task: Natural Questions (NQ Open) — evaluate Exact Match (EM)\n",
    "# Model: RAG-Token NQ checkpoint (facebook/rag-token-nq)\n",
    "# Paper mapping: Table 1 “RAG-Token” on NQ; key knob is K retrieved docs (n_docs)\n",
    "# Goal: Reproduce the baseline RAG-Token behavior and EM score with HF checkpoint\n",
    "# ============================================================\n",
    "rag_tok_nq = RAGModelManager(\n",
    "    model_name=\"facebook/rag-token-nq\",\n",
    "    rag_type=\"token\",\n",
    "    n_docs=15,  # Paper-style setting: RAG-Token commonly reported with ~15 docs for NQ test\n",
    "    use_dummy=USE_DUMMY,\n",
    "    index_name=INDEX_NAME,\n",
    "    num_beams=1,  # Greedy decoding for QA\n",
    "    max_new_tokens=16  # Keep outputs short; aligns with EM evaluation\n",
    ")\n",
    "\n",
    "print(\"Experiment: Natural Questions (RAG-Token baseline reproduction)\")\n",
    "nq_score_tok = run_experiment(\"Natural Questions\", rag_tok_nq, num_samples=NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e6a6bd-38ef-4150-a51d-aeb33689b490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model: WQ_models/facebook_rag-sequence-nq__sequence__wq_ft__nDocs10__20260118_124326 (sequence)...\n",
      "Model loaded successfully.\n",
      "Configured n_docs=15, num_beams=1, max_new_tokens=16\n",
      "Experiment: WebQuestions (RAG-Sequence baseline reproduction)\n",
      "\n",
      "========================================\n",
      "STARTING EXPERIMENT: WebQuestions\n",
      "========================================\n",
      "Processing WebQuestions dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████| 2032/2032 [00:00<00:00, 36784.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2032 TOTAL samples for WebQuestions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 254/254 [2:22:05<00:00, 33.57s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS for WebQuestions:\n",
      "Exact Match (EM): 37.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Task: WebQuestions (WQ) — evaluate Exact Match (EM)\n",
    "# Model: Fine-tuned RAG-Sequence checkpoint (local folder path via FT_WQ_SEQ_DIR)\n",
    "# Paper mapping: Table 1 “RAG-Seq.” on WQ; paper initializes WQ from an NQ-trained RAG model, then fine-tunes\n",
    "# Goal: Reproduce WQ EM after task-specific fine-tuning (or, if missing, run a transfer baseline using NQ checkpoint)\n",
    "# ============================================================\n",
    "FT_WQ_SEQ_DIR = \"WQ_models/facebook_rag-sequence-nq__sequence__wq_ft__nDocs10__20260118_124326\"\n",
    "rag_seq_ft_wq = None\n",
    "\n",
    "if FT_WQ_SEQ_DIR:\n",
    "    rag_seq_ft_wq = RAGModelManager(\n",
    "        model_name=FT_WQ_SEQ_DIR,\n",
    "        rag_type=\"sequence\",\n",
    "        use_dummy=USE_DUMMY,\n",
    "        index_name=INDEX_NAME,\n",
    "        n_docs=15,\n",
    "        num_beams=1,\n",
    "        max_new_tokens=16\n",
    "    )\n",
    "print(\"Experiment: WebQuestions (RAG-Sequence baseline reproduction)\")\n",
    "if rag_seq_ft_wq:\n",
    "    wb_score_seq = run_experiment(\"WebQuestions\", rag_seq_ft_wq, num_samples=NUM_SAMPLES)\n",
    "else:\n",
    "    print(\"No FT_WQ_SEQ_DIR set -> Evaluating NQ sequence model on WQ (expecting low EM)\")\n",
    "    wb_score_seq = run_experiment(\"WebQuestions\", rag_seq_nq, num_samples=NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "251c8c71-8b96-4eaa-bc1b-0fe8e8efebbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model: WQ_models/facebook_rag-token-nq__token__wq_ft__nDocs10__20260117_163413 (token)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "/opt/micromamba/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Configured n_docs=15, num_beams=1, max_new_tokens=16\n",
      "Experiment: WebQuestions (RAG-Token baseline reproduction)\n",
      "\n",
      "========================================\n",
      "STARTING EXPERIMENT: WebQuestions\n",
      "========================================\n",
      "Processing WebQuestions dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████| 2032/2032 [00:00<00:00, 39253.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2032 TOTAL samples for WebQuestions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/254 [00:00<?, ?it/s]/opt/micromamba/lib/python3.11/site-packages/transformers/generation/utils.py:2465: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.\n",
      "  warnings.warn(\n",
      "Generating: 100%|██████████| 254/254 [35:11<00:00,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS for WebQuestions:\n",
      "Exact Match (EM): 32.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Task: WebQuestions (WQ) — evaluate Exact Match (EM)\n",
    "# Model: Fine-tuned RAG-Token checkpoint (local folder path via FT_WQ_TOKEN_DIR)\n",
    "# Paper mapping: Table 1 “RAG-Token” on WQ; paper initializes WQ from NQ RAG, then fine-tunes and evaluates EM\n",
    "# Goal: Reproduce WQ EM for a token-level RAG model after fine-tuning (or, if missing, run a transfer baseline using NQ token checkpoint)\n",
    "# ============================================================\n",
    "FT_WQ_TOKEN_DIR = \"WQ_models/facebook_rag-token-nq__token__wq_ft__nDocs10__20260117_163413\"\n",
    "rag_tok_ft_wq = None\n",
    "\n",
    "if FT_WQ_TOKEN_DIR:\n",
    "    rag_tok_ft_wq = RAGModelManager(\n",
    "        model_name=FT_WQ_TOKEN_DIR,\n",
    "        rag_type=\"token\",\n",
    "        use_dummy=USE_DUMMY,\n",
    "        index_name=INDEX_NAME,\n",
    "        n_docs=15,\n",
    "        num_beams=1,\n",
    "        max_new_tokens=16\n",
    "    )\n",
    "print(\"Experiment: WebQuestions (RAG-Token baseline reproduction)\")\n",
    "if rag_tok_ft_wq:\n",
    "    wb_score_tok = run_experiment(\"WebQuestions\", rag_tok_ft_wq, num_samples=NUM_SAMPLES)\n",
    "else:\n",
    "    print(\"No FT_WQ_TOKEN_DIR set -> Evaluating NQ token model on WQ (expecting low EM)\")\n",
    "    wb_score_tok = run_experiment(\"WebQuestions\", rag_tok_nq, num_samples=NUM_SAMPLES)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
